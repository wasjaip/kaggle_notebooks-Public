{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":57891,"databundleVersionId":7056235,"sourceType":"competition"}],"dockerImageVersionId":30554,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### –û–¥–∏–Ω –∏–∑ –≤–∞—Ä–∏–∞–Ω—Ç —Ä–µ—à–µ–Ω–∏–µ –∑–∞–¥–∞–∏ –ø–æ Optiver - Trading at the Close —Å —Ö–æ—Ä–æ—à–∏–º –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–º Mean Absolute Error (MAE) ","metadata":{}},{"cell_type":"markdown","source":"# –í–≤–µ–¥–µ–Ω–∏–µ\n–≠—Ç–æ—Ç –±–ª–æ–∫–Ω–æ—Ç  —Å –Ω–∞—à–µ–π –∞–Ω—Å–∞–º–±–ª–µ–≤–æ–π –º–æ–¥–µ–ª—å—é LGBM –∏ CatBoost —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–µ—Ç–æ–¥–∞ –æ—Ç–ª–æ–∂–µ–Ω–Ω–æ–π –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏ (holdout CV) –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏—è –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Ä–µ–π—Ç–∏–Ω–≥–∞ –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ (–ø—Ä–∏–º–µ—Ä–Ω–æ —á–µ—Ä–µ–∑ 40 –¥–Ω–µ–π –ø–æ—Å–ª–µ date_id 480).\n\n\n–û—Å–æ–±–∞—è –±–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç—å –∞–≤—Ç–æ—Ä–∞–º —Å–ª–µ–¥—É—é—â–∏—Ö –æ–±—â–µ–¥–æ—Å—Ç—É–ø–Ω—ã—Ö –±–ª–æ–∫–Ω–æ—Ç–æ–≤ –∑–∞ –∏—Ö –Ω–µ–æ—Ü–µ–Ω–∏–º—ã–π –≤–∫–ª–∞–¥:\n\n@lblhandsome, \"‚ö°Optiver üöÄRobust Best ‚ö° Single Model\"\n\n@zulqarnainali, \"LGB Fine-tuned üöÄ(Explained)\"\n\n@verracodeguacas, \"üñêÔ∏è-Fold CV üöÄ\"\n\n@yekenot, \"Feature Elimination by CatBoost\"\n\n@jirkaborovec, \"üìàOptiverüìâ: Feature Eng & Optunaüå™Ô∏èLightGBM@GPU\"\n\n@yunchonggan,\"Weights of the Synthetic Index\"\n\n–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Å–ª–µ–¥—É–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU T4x2. GPU P100 –º–æ–∂–µ—Ç –≤—ã–∑–≤–∞—Ç—å –ø—Ä–æ–±–ª–µ–º—ã —Å –ø–∞–º—è—Ç—å—é –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ Catboost –Ω–∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º–µ Kaggle. –ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ —Å–º. –≤ —ç—Ç–æ–º –±–ª–æ–∫–Ω–æ—Ç–µ https://www.kaggle.com/code/yekenot/feature-elimination-by-catboost\n","metadata":{}},{"cell_type":"markdown","source":"# libraries","metadata":{}},{"cell_type":"code","source":"import gc  # –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–±–æ—Ä–∫–æ–π –º—É—Å–æ—Ä–∞ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞–º—è—Ç—å—é\nimport os  # –§—É–Ω–∫—Ü–∏–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º–æ–π\nimport time  # –§—É–Ω–∫—Ü–∏–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º\nimport warnings  # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π\nfrom itertools import combinations  # –î–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–º–±–∏–Ω–∞—Ü–∏–π —ç–ª–µ–º–µ–Ω—Ç–æ–≤\nfrom warnings import simplefilter  # –£–ø—Ä–æ—â–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π\n\n#  –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è\nimport joblib  # –î–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π\nimport xgboost as xgb\nimport lightgbm as lgb  # –§—Ä–µ–π–º–≤–æ—Ä–∫ LightGBM –¥–ª—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –±—É—Å—Ç–∏–Ω–≥–∞\nimport catboost as ctb\nimport numpy as np  # –ß–∏—Å–ª–µ–Ω–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏\nimport pandas as pd  # –ú–∞–Ω–∏–ø—É–ª—è—Ü–∏—è –∏ –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö\n# from pandarallel import pandarallel\nfrom sklearn.metrics import mean_absolute_error  # –ú–µ—Ç—Ä–∏–∫–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏\nfrom sklearn.model_selection import KFold, TimeSeriesSplit  # –¢–µ—Ö–Ω–∏–∫–∏ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏\n\n# –û—Ç–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π –¥–ª—è —á–∏—Å—Ç–æ—Ç—ã –∫–æ–¥–∞\nwarnings.filterwarnings(\"ignore\")\nsimplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n# pandarallel.initialize(nb_workers=4)\n\n\nmax_lookback = np.nan  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –≤–æ–∑–≤—Ä–∞—Ç–∞ (–Ω–µ —É–∫–∞–∑–∞–Ω)\nsplit_day = 435  # –î–µ–Ω—å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –¥–ª—è –¥–∞–Ω–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤\nN_STOCKS = 200  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–∫—Ü–∏–π\nMAX_N_NEIGHBOURS = 10  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ—Å–µ–¥–µ–π\nNEIGHBOUR_CORR_THRESHOLD = 0.3","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:18:39.075005Z","iopub.execute_input":"2023-12-12T17:18:39.075904Z","iopub.status.idle":"2023-12-12T17:18:39.084333Z","shell.execute_reply.started":"2023-12-12T17:18:39.075868Z","shell.execute_reply":"2023-12-12T17:18:39.083255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# is_offline = True  # –§–ª–∞–≥ –¥–ª—è —Ä–µ–∂–∏–º–∞ –æ–Ω–ª–∞–π–Ω/–æ—Ñ—Ñ–ª–∞–π–Ω\n# is_train = False  # –§–ª–∞–≥ –¥–ª—è —Ä–µ–∂–∏–º–∞ –æ–±—É—á–µ–Ω–∏—è\n# is_infer = False  # –§–ª–∞–≥ –¥–ª—è —Ä–µ–∂–∏–º–∞ –≤—ã–≤–æ–¥–∞","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA ","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/optiver-trading-at-the-close/train.csv\")\n\n# –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ —Å –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –≤ —Å—Ç–æ–ª–±—Ü–µ \"target\"\ndf = df.dropna(subset=[\"target\"])\n# –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏, –≥–¥–µ –≤ —Å—Ç–æ–ª–±—Ü–µ \"target\" –µ—Å—Ç—å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è.\n\ndf = df.dropna(subset=[\"wap\"])\n# –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏, –≥–¥–µ –≤ —Å—Ç–æ–ª–±—Ü–µ \"wap\" –µ—Å—Ç—å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è.\n\n# –°–±—Ä–æ—Å –∏–Ω–¥–µ–∫—Å–∞ DataFrame –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π –Ω–∞ –º–µ—Å—Ç–µ\ndf.reset_index(drop=True, inplace=True)\n# –°–±—Ä–∞—Å—ã–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å, —á—Ç–æ–±—ã –æ–Ω —à–µ–ª –æ—Ç 0 –¥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–æ–∫ - 1, –∏ –¥–µ–ª–∞–µ–º —ç—Ç–æ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –≤ —Å–∞–º–æ–º DataFrame (inplace=True).\n\n# –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ DataFrame (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –∏ —Å—Ç–æ–ª–±—Ü–æ–≤)\ndf_shape = df.shape\n# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä DataFrame (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –∏ —Å—Ç–æ–ª–±—Ü–æ–≤) –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π df_shape.\n\ndf_train = df\n# –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π DataFrame –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π df_train –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.\n","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:18:39.099104Z","iopub.execute_input":"2023-12-12T17:18:39.099508Z","iopub.status.idle":"2023-12-12T17:18:58.440409Z","shell.execute_reply.started":"2023-12-12T17:18:39.099468Z","shell.execute_reply":"2023-12-12T17:18:58.439504Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è","metadata":{}},{"cell_type":"code","source":"# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ DataFrame Pandas\ndef reduce_mem_usage(df, verbose=0):\n    \"\"\"\n    –ü—Ä–æ—Ö–æ–¥–∏—Ç —á–µ—Ä–µ–∑ –≤—Å–µ —á–∏—Å–ª–æ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã DataFrame –∏ –∏–∑–º–µ–Ω—è–µ—Ç –∏—Ö —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö\n    –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏.\n    \"\"\"\n    \n    # –†–∞—Å—á–µ—Ç –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ DataFrame\n    start_mem = df.memory_usage().sum() / 1024**2\n\n    # –ü—Ä–æ—Ö–æ–¥ –ø–æ –∫–∞–∂–¥–æ–º—É —Å—Ç–æ–ª–±—Ü—É –≤ DataFrame\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        # –ü—Ä–æ–≤–µ—Ä–∫–∞, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–∞ 'object' (—Ç.–µ., —á–∏—Å–ª–æ–≤–æ–π)\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            \n            # –ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–∞ —Ü–µ–ª–æ—á–∏—Å–ª–µ–Ω–Ω—ã–º\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                # –ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–∞ —Ç–∏–ø–æ–º —Å –ø–ª–∞–≤–∞—é—â–µ–π —Ç–æ—á–∫–æ–π\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float32)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float32)\n\n    # –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏, –µ—Å–ª–∏ 'verbose' –∏—Å—Ç–∏–Ω–∞\n    if verbose:\n        logger.info(f\"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ dataframe —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç {start_mem:.2f} MB\")\n        end_mem = df.memory_usage().sum() / 1024**2\n        logger.info(f\"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {end_mem:.2f} MB\")\n        decrease = 100 * (start_mem - end_mem) / start_mem\n        logger.info(f\"–£–º–µ–Ω—å—à–µ–Ω–æ –Ω–∞ {decrease:.2f}%\")\n\n    # –í–æ–∑–≤—Ä–∞—â–µ–Ω–∏–µ DataFrame —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–∞–º—è—Ç–∏\n    return df\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:18:58.442757Z","iopub.execute_input":"2023-12-12T17:18:58.443076Z","iopub.status.idle":"2023-12-12T17:18:58.458166Z","shell.execute_reply.started":"2023-12-12T17:18:58.443049Z","shell.execute_reply":"2023-12-12T17:18:58.457248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π —Ä–∞—Å—á–µ—Ç —Ç—Ä–∏–ø–ª–µ—Ç–Ω–æ–≥–æ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ —Å –ø–æ–º–æ—â—å—é Numba\n\nNumba - —ç—Ç–æ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω–∞—è –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∫–æ–¥–∞ –Ω–∞ —è–∑—ã–∫–µ Python –ø—É—Ç–µ–º –∫–æ–º–ø–∏–ª—è—Ü–∏–∏ —Ñ—É–Ω–∫—Ü–∏–π –≤ –º–∞—à–∏–Ω–Ω—ã–π –∫–æ–¥. –û–Ω–∞ —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å —á–∏—Å–ª–æ–≤—ã–º–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è–º–∏.\n\n–ß—Ç–æ–±—ã –ø—Ä–æ–≤–µ—Å—Ç–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Numba, –≤—ã –º–æ–∂–µ—Ç–µ –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä–æ–º @njit (–¥–ª—è Just-In-Time –∫–æ–º–ø–∏–ª—è—Ü–∏–∏) –∏ —É–∫–∞–∑–∞—Ç—å –∞—Ç—Ä–∏–±—É—Ç parallel=True. –í–æ—Ç –ø—Ä–∏–º–µ—Ä, –∫–∞–∫ —ç—Ç–æ –º–æ–∂–µ—Ç –≤—ã–≥–ª—è–¥–µ—Ç—å:\n\npython\n'''\n\n        import numpy as np\n        from numba import njit, prange\n        \n    @njit(parallel=True)\n    def calculate_triplet_imbalance(data):\n        num_elements = data.shape[0]\n        result = np.zeros(num_elements)\n\n    for i in prange(num_elements):\n        # –í–∞—à –∫–æ–¥ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Ç—Ä–∏–ø–ª–µ—Ç–Ω–æ–≥–æ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞\n        # –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ data[i] –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ —ç–ª–µ–º–µ–Ω—Ç–∞–º –º–∞—Å—Å–∏–≤–∞\n\n    return result\n\n    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n    data_array = np.random.rand(1000, 3)  # –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –≤–∞—à –º–∞—Å—Å–∏–≤ –¥–∞–Ω–Ω—ã—Ö\n    result_array = calculate_triplet_imbalance(data_array)\n'''","metadata":{}},{"cell_type":"code","source":"# –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ Numba –¥–ª—è –∫–æ–º–ø–∏–ª—è—Ü–∏–∏ \"–Ω–∞ –ª–µ—Ç—É\" (JIT) –∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏\nfrom numba import njit, prange\n\n# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ —Ç—Ä–æ–π–Ω—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π –≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–º —Ä–µ–∂–∏–º–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Numba\n@njit(parallel=True)\ndef compute_triplet_imbalance(df_values, comb_indices):\n    num_rows = df_values.shape[0]\n    num_combinations = len(comb_indices)\n    imbalance_features = np.empty((num_rows, num_combinations))\n\n    # –¶–∏–∫–ª –ø–æ –≤—Å–µ–º –∫–æ–º–±–∏–Ω–∞—Ü–∏—è–º –∏–∑ —Ç—Ä–µ—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤\n    for i in prange(num_combinations):\n        a, b, c = comb_indices[i]\n        \n        # –¶–∏–∫–ª –ø–æ —Å—Ç—Ä–æ–∫–∞–º DataFrame\n        for j in range(num_rows):\n            # –ù–∞—Ö–æ–¥–∏–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ, –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∏ —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ —Ç—Ä–µ—Ö –∑–Ω–∞—á–µ–Ω–∏–π\n            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n            \n            # –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å\n            if mid_val == min_val:\n                imbalance_features[j, i] = np.nan\n            else:\n                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n\n    return imbalance_features","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# üìä Function to compute triplet imbalance in parallel using Numba\n@njit(parallel=True)\ndef compute_triplet_imbalance(df_values, comb_indices):\n    num_rows = df_values.shape[0]\n    num_combinations = len(comb_indices)\n    imbalance_features = np.empty((num_rows, num_combinations))\n\n    # üîÅ Loop through all combinations of triplets\n    for i in prange(num_combinations):\n        a, b, c = comb_indices[i]\n        \n        # üîÅ Loop through rows of the DataFrame\n        for j in range(num_rows):\n\n            if df_values[j, a] < df_values[j, b]:\n                min_val = df_values[j, a]\n                max_val = df_values[j, b]\n            else:\n                max_val = df_values[j, a]\n                min_val = df_values[j, b]\n\n            if min_val < df_values[j, c]:\n                if df_values[j, c] < max_val:\n                    mid_val = df_values[j, c]\n                else:\n                    mid_val = max_val\n                    max_val = df_values[j, c]\n            else:\n                mid_val = min_val\n                min_val = df_values[j, c]\n            \n            # üö´ Prevent division by zero\n            if max_val == min_val:\n                imbalance_features[j, i] = np.nan\n            elif mid_val == min_val:\n                imbalance_features[j, i] = np.nan\n            else:\n                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n    \n    return imbalance_features","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ —Ç—Ä–æ–π–Ω—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π –ø–æ –¥–∞–Ω–Ω—ã–º –æ —Ü–µ–Ω–∞—Ö –∏ DataFrame\ndef calculate_triplet_imbalance_numba(price, df):\n    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ DataFrame –≤ –º–∞—Å—Å–∏–≤ numpy –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å Numba\n    df_values = df[price].values\n    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n\n    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ —Ç—Ä–æ–π–Ω—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ Numba\n    features_array = compute_triplet_imbalance(df_values, comb_indices)\n\n    # –°–æ–∑–¥–∞–Ω–∏–µ DataFrame –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n    features = pd.DataFrame(features_array, columns=columns)\n\n    return features","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# –Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ","metadata":{}},{"cell_type":"code","source":"# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞\ndef imbalance_features(df):\n\n    stock_groups = df.groupby([\"date_id\", \"seconds_in_bucket\"])\n    # –ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π WAP\n    df[\"wwap\"] = df.stock_id.map(weights) * df.wap\n    df[\"iwap\"] = stock_groups[\"wwap\"].transform(lambda x: x.sum())\n    del df[\"wwap\"]\n\n    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ø–∏—Å–∫–æ–≤ –Ω–∞–∑–≤–∞–Ω–∏–π —Å—Ç–æ–ª–±—Ü–æ–≤, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å —Ü–µ–Ω–∞–º–∏ –∏ —Ä–∞–∑–º–µ—Ä–∞–º–∏\n    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n\n    # V1 –ø—Ä–∏–∑–Ω–∞–∫–∏\n    # –†–∞—Å—á–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ñ—É–Ω–∫—Ü–∏–∏ Pandas eval\n    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n    df[\"mid_price\"] = df.eval(\"(ask_price + bid_price) / 2\")\n    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n    df[\"matched_imbalance\"] = df.eval(\"(imbalance_size-matched_size)/(matched_size+imbalance_size)\")\n    df[\"all_size\"] = df.eval(\"matched_size + imbalance_size\") # –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ\n    df[\"imbalance_size_for_buy_sell\"] = df.eval(\"imbalance_size * imbalance_buy_sell_flag\")  # –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ\n    \n    cols = ['wap', 'imbalance_size_for_buy_sell', \"bid_size\", \"ask_size\"]\n    for q in [0.25, 0.5, 0.75]:  # –ò—Å–ø—ã—Ç–∞–Ω–∏–µ –±–æ–ª—å—à–µ–≥–æ/—Ä–∞–∑–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ q\n        df[[f'{col}_quantile_{q}' for col in cols]] = stock_groups[cols].transform(lambda x: x.quantile(q)).astype(np.float32)\n    \n    # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø–æ–ø–∞—Ä–Ω—ã—Ö –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–≤ —Ü–µ–Ω\n    for c in combinations(prices, 2):\n        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\").astype(np.float32)\n\n    for c in combinations(sizes, 2):\n        df[f\"{c[0]}/{c[1]}\"] = df.eval(f\"({c[0]})/({c[1]})\").astype(np.float32)\n\n    # –†–∞—Å—á–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ —Ç—Ä–æ–π–Ω—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ Numba\n    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n        df[triplet_feature.columns] = triplet_feature.values.astype(np.float32)\n        \n    # V2 –ø—Ä–∏–∑–Ω–∞–∫–∏\n    # –†–∞—Å—á–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n    stock_groups = df.groupby(['stock_id', 'date_id'])\n    df[\"imbalance_momentum\"] = stock_groups['imbalance_size'].diff(periods=1) / df['matched_size']\n    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n    df[\"spread_intensity\"] = stock_groups['price_spread'].diff()\n    df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n    df['wap_advantage'] = df.wap - df.iwap  # –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ\n\n    # –†–∞—Å—á–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n    df_prices = df[prices]\n    df_sizes = df[sizes]\n    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n        df[f\"all_prices_{func}\"] = df_prices.agg(func, axis=1)\n        df[f\"all_sizes_{func}\"] = df_sizes.agg(func, axis=1)\n        \n    # V3 –ø—Ä–∏–∑–Ω–∞–∫–∏\n    # –†–∞—Å—á–µ—Ç —Å–¥–≤–∏–Ω—É—Ç—ã—Ö –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤\n    cols = ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag', \"wap\", \"iwap\"]\n    stock_groups_cols = stock_groups[cols]\n    for window in [1, 2, 3, 6, 10]:\n        df[[f\"{col}_shift_{window}\" for col in cols]] = stock_groups_cols.shift(window)\n\n    cols = ['matched_size', 'imbalance_size', 'reference_price', \"iwap\"] #wap\n    stock_groups_cols = stock_groups[cols]\n    for window in [1, 2, 3, 6, 10]:\n        df[[f\"{col}_ret_{window}\" for col in cols]] = stock_groups_cols.pct_change(window).astype(np.float32)\n\n    # –†–∞—Å—á–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Ä–∞–∑–Ω–∏—Ü—ã –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤\n    cols = ['ask_price', 'bid_price', 'ask_size', 'bid_size', 'wap', 'near_price', 'far_price', 'imbalance_size_for_buy_sell']\n    stock_groups_cols = stock_groups[cols]\n    for window in [1, 2, 3, 6, 10]:\n        df[[f\"{col}_diff_{window}\" for col in cols]] = stock_groups_cols.diff(window).astype(np.float32)\n\n    # V4 –ø—Ä–∏–∑–Ω–∞–∫–∏\n    # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∞ `time_since_last_imbalance_change`\n    df['flag_change'] = stock_groups['imbalance_buy_sell_flag'].diff().ne(0).astype(int)\n    # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ cumsum –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –≥—Ä—É–ø–ø—ã, —É–≤–µ–ª–∏—á–∏–≤–∞—é—â–µ–≥–æ—Å—è –ø—Ä–∏ –∫–∞–∂–¥–æ–º –∏–∑–º–µ–Ω–µ–Ω–∏–∏ —Ñ–ª–∞–≥–∞\n    df['group'] = df.groupby(['stock_id', 'date_id'])['flag_change'].cumsum()\n    # –†–∞—Å—á–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ —Å –º–æ–º–µ–Ω—Ç–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ñ–ª–∞–≥–∞ –≤ –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø–µ\n    group_min = df.groupby(['stock_id', 'date_id', 'group'])['seconds_in_bucket'].transform('min')\n    df['time_since_last_imbalance_change'] = df['seconds_in_bucket'] - group_min\n    # –û–±–Ω—É–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –≤ –º–µ—Å—Ç–∞—Ö, –≥–¥–µ –ø—Ä–æ–∏–∑–æ—à–ª–æ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ñ–ª–∞–≥–∞\n    df['time_since_last_imbalance_change'] *= (1 - df['flag_change'])\n    df.drop(columns=['flag_change', 'group'], inplace=True)\n    \n    cols = ['imbalance_size_for_buy_sell']\n    stock_groups_cols = stock_groups[cols]\n    for window in [5, 10]:\n        mean_col = stock_groups_cols.transform(lambda x: x.rolling(window=window).mean())\n        std_col = stock_groups_cols.transform(lambda x: x.rolling(window=window).std())\n        df[[f'z_score_{col}_{window}' for col in cols]] = (df[cols] - mean_col) / std_col\n    \n    # –ó–∞–º–µ–Ω–∞ –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ 0\n    return df.replace([np.inf, -np.inf], 0)\n\n# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏ –∞–∫—Ü–∏–æ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\ndef other_features(df):\n    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60  # –°–µ–∫—É–Ω–¥—ã\n    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60  # –ú–∏–Ω—É—Ç—ã\n\n    # –ü—Ä–∏—Å–≤–æ–µ–Ω–∏–µ –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∫ DataFrame\n    for key, value in global_stock_id_feats.items():\n        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n        \n    for key, value in global_seconds_feats.items():\n        df[f\"global_seconds_{key}\"] = df[\"seconds_in_bucket\"].map(value.to_dict())\n\n    return df\n\n           \n","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:18:59.240287Z","iopub.execute_input":"2023-12-12T17:18:59.240593Z","iopub.status.idle":"2023-12-12T17:18:59.28331Z","shell.execute_reply.started":"2023-12-12T17:18:59.240566Z","shell.execute_reply":"2023-12-12T17:18:59.282367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"–¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –¥–Ω–µ–π, –≤–∫–ª—é—á–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –¥–∞–Ω–Ω—ã—Ö, —Å–¥–≤–∏–≥ —Å—Ç–æ–ª–±—Ü–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –í —Ñ—É–Ω–∫—Ü–∏–∏ generate_all_features –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–≥–æ –≤ –∞–Ω–∞–ª–∏–∑–µ –¥–∞–Ω–Ω—ã—Ö.","metadata":{}},{"cell_type":"code","source":"def last_days_features(df: pd.DataFrame, feat_last=None, target_last=None):\n    size = None\n    \n    # –ï—Å–ª–∏ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω—ã –∏ –∏—Ö –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–æ–ª—å—à–µ –Ω—É–ª—è\n    if feat_last is not None and len(feat_last) > 0:\n        # –í—ã–±–æ—Ä —Å—Ç–æ–ª–±—Ü–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–∞–∫ –≤ —Ç–µ–∫—É—â–µ–º, —Ç–∞–∫ –∏ –≤ –ø—Ä–µ–¥—ã–¥—É—â–µ–º DataFrame\n        cols = [col for col in df.columns if col in set(feat_last.columns)]\n        # –ï—Å–ª–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω—ã —Ü–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n        if target_last is not None:\n            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞ –∏ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏\n            cols.append(\"target\")\n            feat_last[\"target\"] = target_last\n            df[\"target\"] = 0\n        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –¥–æ–±–∞–≤–ª–µ–Ω–∏—é –∑–∞–ø–æ–ª–Ω–∏—Ç–µ–ª–µ–π\n        paddings = []\n        # –ù–∞—Ö–æ–∂–¥–µ–Ω–∏–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è —Å–µ–∫—É–Ω–¥ –≤ —Ç–µ–∫—É—â–µ–º DataFrame\n        second_start = df.seconds_in_bucket.max()\n        # –ü–æ–ª—É—á–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–ø–æ–ª–Ω–∏—Ç–µ–ª–µ–π\n        padding_src = df[df.seconds_in_bucket == second_start]\n        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤ —Ç–µ–∫—É—â–µ–≥–æ –∏ –∑–∞–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ DataFrame\n        size = len(df)\n        size_pad = len(padding_src) * 6\n        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–∞–ø–æ–ª–Ω–∏—Ç–µ–ª–µ–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞ –≤ 10 —Å–µ–∫—É–Ω–¥\n        for second in range(second_start + 10, second_start + 70, 10):\n            padding = padding_src.copy()\n            padding[\"seconds_in_bucket\"] = second\n            paddings.append(padding)\n        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, —Ç–µ–∫—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –∑–∞–ø–æ–ª–Ω–∏—Ç–µ–ª–µ–π\n        df = pd.concat([feat_last[cols], df] + paddings)\n\n    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –¥–Ω–µ–π\n    # TODO: –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –±–æ–ª—å—à–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n    cols = ['near_price', 'far_price', 'depth_pressure']\n    if 'target' in df.columns:\n        cols.append('target')\n    stock_groups = df.groupby(['stock_id', 'seconds_in_bucket'])\n    stock_groups_cols = stock_groups[cols]\n    # –°–¥–≤–∏–≥ —Å—Ç–æ–ª–±—Ü–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n    for window in [1]:\n        df[[f\"{col}_last_{window}day\" for col in cols]] = stock_groups_cols.shift(window)\n    if cols[-1] == \"target\":\n        cols.pop()\n    \n    cols = [f\"{col}_last_{window}day\" for col in cols]\n    stock_groups = df.groupby(['stock_id', 'date_id'])\n    stock_groups_cols = stock_groups[cols]\n    # –°–¥–≤–∏–≥ —Å—Ç–æ–ª–±—Ü–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –±—É–¥—É—â–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n    for window in [1, 2, 3, 6]:\n        df[[f\"{col}_future_{window}\" for col in cols]] = stock_groups_cols.shift(-window)\n        \n    if size:\n        return df[-(size + size_pad):-size_pad]\n    return df\n\n# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø—É—Ç–µ–º –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –∏ –¥—Ä—É–≥–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\ndef generate_all_features(df, feat_last=None, target_last=None):\n    # –í—ã–±–æ—Ä —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n    cols = [c for c in df.columns if c not in {\"row_id\", \"time_id\", \"currently_scored\"}]\n    df = df[cols]\n    \n    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞\n    df = imbalance_features(df)\n    \n    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –¥–Ω–µ–π\n    df = last_days_features(df, feat_last, target_last)\n    \n    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏ –∞–∫—Ü–∏–æ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n    df = other_features(df)\n\n    gc.collect()  # –û—Å—É—â–µ—Å—Ç–≤–ª–µ–Ω–∏–µ —Å–±–æ—Ä–∫–∏ –º—É—Å–æ—Ä–∞ –¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è –ø–∞–º—è—Ç–∏\n    \n    # –í—ã–±–æ—Ä –∏ –≤–æ–∑–≤—Ä–∞—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n    feature_name = [i for i in df.columns if i not in {\"row_id\", \"target\", \"time_id\"}]\n    \n    return df[feature_name]\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## –í–ï—Å–∞ ","metadata":{}},{"cell_type":"code","source":"weights = [\n    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n]\n\nweights = {int(k):v for k,v in enumerate(weights)}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"–æ–∑–¥–∞–Ω–∏–µ –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –ø–æ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—É –∞–∫—Ü–∏–π –∏ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö","metadata":{}},{"cell_type":"code","source":"# –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞ –ø–æ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—É –∞–∫—Ü–∏–π\nstock_group = df_train.groupby(\"stock_id\")\n\n# –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞ \"imbalance_size_for_buy_sell\" –ø—É—Ç–µ–º —É–º–Ω–æ–∂–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –Ω–∞ —Ñ–ª–∞–≥ –ø–æ–∫—É–ø–∫–∏/–ø—Ä–æ–¥–∞–∂–∏\ndf_train[\"imbalance_size_for_buy_sell\"] = df_train.eval(\"imbalance_size * imbalance_buy_sell_flag\")\n\n# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—É –∞–∫—Ü–∏–π\nglobal_stock_id_feats = {\n    \"median_size\": stock_group[\"bid_size\"].median() + stock_group[\"ask_size\"].median(),  # –ú–µ–¥–∏–∞–Ω–∞ —Ä–∞–∑–º–µ—Ä–∞ –∑–∞—è–≤–æ–∫ –ø–æ–∫—É–ø–∫–∏ –∏ –ø—Ä–æ–¥–∞–∂–∏\n    \"std_size\": stock_group[\"bid_size\"].std() + stock_group[\"ask_size\"].std(),  # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –∑–∞—è–≤–æ–∫ –ø–æ–∫—É–ø–∫–∏ –∏ –ø—Ä–æ–¥–∞–∂–∏\n    \"ptp_size\": stock_group[\"bid_size\"].max() - stock_group[\"bid_size\"].min(),  # –†–∞–∑–º–∞—Ö —Ä–∞–∑–º–µ—Ä–∞ –∑–∞—è–≤–æ–∫ –ø–æ–∫—É–ø–∫–∏\n    \"median_price\": stock_group[\"bid_price\"].median() + stock_group[\"ask_price\"].median(),  # –ú–µ–¥–∏–∞–Ω–∞ —Ü–µ–Ω –∑–∞—è–≤–æ–∫ –ø–æ–∫—É–ø–∫–∏ –∏ –ø—Ä–æ–¥–∞–∂–∏\n    \"std_price\": stock_group[\"bid_price\"].std() + stock_group[\"ask_price\"].std(),  # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ —Ü–µ–Ω –∑–∞—è–≤–æ–∫ –ø–æ–∫—É–ø–∫–∏ –∏ –ø—Ä–æ–¥–∞–∂–∏\n    \"ptp_price\": stock_group[\"bid_price\"].max() - stock_group[\"ask_price\"].min(),  # –†–∞–∑–º–∞—Ö —Ü–µ–Ω –∑–∞—è–≤–æ–∫ –ø–æ–∫—É–ø–∫–∏ –∏ –ø—Ä–æ–¥–∞–∂–∏\n    \"median_far_price\": stock_group[\"far_price\"].median(),  # –ú–µ–¥–∏–∞–Ω–∞ –¥–∞–ª—å–Ω–µ–π —Ü–µ–Ω—ã\n    \"median_near_price\": stock_group[\"near_price\"].median(),  # –ú–µ–¥–∏–∞–Ω–∞ –±–ª–∏–∂–Ω–µ–π —Ü–µ–Ω—ã\n    \"median_imbalance_size_for_buy_sell\": stock_group[\"imbalance_size_for_buy_sell\"].median(),  # –ú–µ–¥–∏–∞–Ω–∞ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ —Ä–∞–∑–º–µ—Ä–∞ –¥–ª—è –ø–æ–∫—É–ø–∫–∏/–ø—Ä–æ–¥–∞–∂–∏\n    \"matched_size\":stock_group[\"matched_size\"].median(),  # –ú–µ–¥–∏–∞–Ω–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞\n}\n\n# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ —Å–µ–∫—É–Ω–¥–∞–º\nglobal_seconds_feats = {\n    # –ú–µ–¥–∏–∞–Ω–∞ —Ü–µ–ª–µ–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —Å–µ–∫—É–Ω–¥–∞–º –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–π –¥–∞—Ç—ã, –∞ –∑–∞—Ç–µ–º –≤ —Ü–µ–ª–æ–º –ø–æ —Å–µ–∫—É–Ω–¥–∞–º\n    \"median_target\": df_train.groupby([\"date_id\", \"seconds_in_bucket\"])[\"target\"].apply(lambda x: x.abs().mean()).reset_index(0, drop=True).groupby(\"seconds_in_bucket\").median(),\n}\n","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:18:59.284558Z","iopub.execute_input":"2023-12-12T17:18:59.284886Z","iopub.status.idle":"2023-12-12T17:19:06.484298Z","shell.execute_reply.started":"2023-12-12T17:18:59.284859Z","shell.execute_reply":"2023-12-12T17:19:06.483441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LGBM \n\n\nLGBM –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑  **@jirkaborovec**, [\"üìàOptiverüìâ: Feature Eng & Optunaüå™Ô∏èLightGBM@GPU\"](https://www.kaggle.com/code/jirkaborovec/optiver-feature-eng-optuna-lightgbm-gpu) ","metadata":{}},{"cell_type":"code","source":"def train_lgb(df_train_feats, targets, split_day):\n\n    feature_name = list(df_train_feats.columns)\n    feature_name.remove(\"date_id\")\n    offline_split = df_train_feats['date_id'] > (split_day - 45)\n    \n\n    stock_group = df_train_feats.groupby(\"stock_id\")\n    global global_stock_id_feats, global_seconds_feats\n    global_stock_id_feats = {\n        \"median_size\": stock_group[\"bid_size\"].median() + stock_group[\"ask_size\"].median(),\n        \"std_size\": stock_group[\"bid_size\"].std() + stock_group[\"ask_size\"].std(),\n        \"ptp_size\": stock_group[\"bid_size\"].max() - stock_group[\"bid_size\"].min(),\n        \"median_price\": stock_group[\"bid_price\"].median() + stock_group[\"ask_price\"].median(),\n        \"std_price\": stock_group[\"bid_price\"].std() + stock_group[\"ask_price\"].std(),\n        \"ptp_price\": stock_group[\"bid_price\"].max() - stock_group[\"ask_price\"].min(),\n        \"median_far_price\": stock_group[\"far_price\"].median(),\n        \"median_near_price\": stock_group[\"near_price\"].median(),\n        \"median_imbalance_size_for_buy_sell\": stock_group[\"imbalance_size_for_buy_sell\"].median(),\n        \"matched_size\": stock_group[\"matched_size\"].median(),\n    }\n    df_train_feats[\"target\"] = targets.values\n    global_seconds_feats = {\n        \"median_target\": df_train_feats.groupby([\"date_id\", \"seconds_in_bucket\"])[\"target\"].apply(lambda x: x.abs().mean()).reset_index(0, drop=True).groupby(\"seconds_in_bucket\").median(),\n    }\n    del df_train_feats[\"target\"]\n    \n    # update global features\n    for key, value in global_stock_id_feats.items():\n        df_train_feats[f\"global_{key}\"] = df_train_feats[\"stock_id\"].map(value.to_dict())\n        \n    for key, value in global_seconds_feats.items():\n        df_train_feats[f\"global_seconds_{key}\"] = df_train_feats[\"seconds_in_bucket\"].map(value.to_dict())\n        \n    lgb_params = {\n        \"objective\": \"mae\",\n        \"n_estimators\": 5500,\n        \"num_leaves\": 465,\n        \"subsample\": 0.65791,\n        \"colsample_bytree\": 0.7,\n        \"learning_rate\": 0.00877,  # 0.00877\n        \"n_jobs\": 4,\n        \"device\": \"gpu\",\n        \"verbosity\": -1,\n        \"importance_type\": \"gain\",\n        \"max_depth\": 14,  # Maximum depth of the tree\n        \"min_child_samples\": 132,  # Minimum number of data points in a leaf\n        \"reg_alpha\": 6,  # L1 regularization term\n        \"reg_lambda\": 0.08,  # L2 regularization term\n    }\n\n    print(f\"Feature length = {len(feature_name)}\")\n\n    # Split data for offline training based on a specific date\n    df_offline_train = df_train_feats[~offline_split]\n    df_offline_valid = df_train_feats[offline_split]\n    df_offline_train_target = targets[~offline_split]\n    df_offline_valid_target = targets[offline_split]\n    \n    \n    train_dataset = lgb.Dataset(df_offline_train[feature_name].values.astype(np.float32), \n                                label=df_offline_train_target.values.astype(np.float32))\n    \n    valid_dataset = lgb.Dataset(df_offline_valid[feature_name].values.astype(np.float32), \n                                label=df_offline_valid_target.values.astype(np.float32))\n    \n    del df_offline_train, df_offline_valid, df_offline_train_target, df_offline_valid_target, offline_split\n\n    print(\"Valid Model Training.\")\n    \n    # Train a LightGBM model on the offline data\n    lgb_model = lgb.LGBMRegressor(**lgb_params)\n  \n    lgb_model.fit(\n        train_dataset.data,\n        train_dataset.label,\n        eval_set=[(valid_dataset.data, valid_dataset.label)],\n        callbacks=[\n            lgb.callback.early_stopping(stopping_rounds=200),\n            lgb.callback.log_evaluation(period=100),\n        ],\n        feature_name=feature_name,\n    )\n    \n    best_iteration_ = lgb_model.best_iteration_\n    \n    # Free up memory by deleting variables\n    del lgb_model\n    gc.collect()\n\n    # Inference\n    df_train_target = targets\n    full_train_dataset = lgb.Dataset(df_train_feats[feature_name].values.astype(np.float32), \n                                     label=df_train_target.values.astype(np.float32))\n    print(\"Infer Model Training.\")\n\n    # Adjust the number of estimators for the inference model\n    infer_params = lgb_params.copy()\n    infer_params[\"n_estimators\"] = int(1.2 * best_iteration_)\n    infer_lgb_model = lgb.LGBMRegressor(**infer_params)\n    \n    infer_lgb_model.fit(\n        full_train_dataset.data,\n        full_train_dataset.label,\n        feature_name=feature_name,\n        )\n    \n#     infer_lgb_model.fit(\n#         df_train_feats[feature_name].values.astype(np.float32), \n#         df_train_target.values.astype(np.float32), \n#         feature_name = feature_name\n#         )\n\n    return infer_lgb_model","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:19:06.49617Z","iopub.execute_input":"2023-12-12T17:19:06.496502Z","iopub.status.idle":"2023-12-12T17:19:06.518112Z","shell.execute_reply.started":"2023-12-12T17:19:06.496475Z","shell.execute_reply":"2023-12-12T17:19:06.517162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Catboost —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ \n\n–°–ø–∞—Å–∏–±–æ  **@yekenot**, [\"Feature Elimination by CatBoost\"](https://www.kaggle.com/code/yekenot/feature-elimination-by-catboost)\n","metadata":{}},{"cell_type":"code","source":"eliminated_features_names = set(['near_price_ask_price_imb',\n                             'iwap',\n                             'all_imbalance_size_for_buy_sell_quantile_0.5',\n                             'iwap_shift_10',\n                             'all_imbalance_size_for_buy_sell_quantile_0.75',\n                             'matched_size_ret_10',\n                             'all_wap_quantile_0.25',\n                             'iwap_shift_6',\n                             'near_price_wap_imb',\n                             'iwap_shift_1',\n                             'iwap_ret_10',\n                             'iwap_ret_6',\n                             'all_imbalance_size_for_buy_sell_quantile_0.25',\n                             'global_ptp_size',\n                             'reference_price_shift_10',\n                             'all_ask_size_quantile_0.75',\n                             'iwap_shift_3',\n                             'iwap_shift_2',\n                             'reference_price_near_price_imb',\n                             'all_bid_size_quantile_0.25',\n                             'global_std_size',\n                             'global_median_price',\n                             'matched_size_ret_6',\n                             'wap_shift_10'])","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:19:06.519207Z","iopub.execute_input":"2023-12-12T17:19:06.519474Z","iopub.status.idle":"2023-12-12T17:19:06.532037Z","shell.execute_reply.started":"2023-12-12T17:19:06.51945Z","shell.execute_reply":"2023-12-12T17:19:06.531179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_ctb(df_train_feats, targets, split_day):\n\n    feature_name = list(df_train_feats.columns)\n    feature_name.remove(\"date_id\")\n    feature_name = [feat for feat in feature_name if feat not in eliminated_features_names]\n    offline_split = df_train_feats['date_id'] > (split_day - 45)\n    \n#     target_mean = targets.values.mean()\n#     pre = df_train_feats[offline_split][[\"date_id\", \"seconds_in_bucket\"]]\n#     pre[\"target\"] = targets[offline_split].values\n    \n    ctb_params = dict(iterations=2000,\n                      learning_rate=1.0,\n                      depth=9,\n                      l2_leaf_reg=30,\n                      bootstrap_type='Bernoulli',\n                      subsample=0.66,\n                      loss_function='MAE',\n                      eval_metric = 'MAE',\n                      metric_period=100,\n                      od_type='Iter',\n                      od_wait=200,\n                      task_type='GPU',\n                      allow_writing_files=False,\n                      random_strength=4.428571428571429\n                      )\n\n    print(f\"Feature length = {len(feature_name)}\")\n\n    # Split data for offline training based on a specific date\n    df_offline_train = df_train_feats[~offline_split]\n    df_offline_valid = df_train_feats[offline_split]\n    df_offline_train_target = targets[~offline_split]\n    df_offline_valid_target = targets[offline_split]\n    \n#     train_dataset = lgb.Dataset(df_offline_train[feature_name].values, label = df_offline_train_target, feature_name = feature_name)\n#     valid_dataset = lgb.Dataset(df_offline_valid[feature_name].values, label = df_offline_valid_target, feature_name = feature_name)\n\n    print(\"Valid Model Training.\")\n\n    # Train a LightGBM model on the offline data\n    ctb_model = ctb.CatBoostRegressor(**ctb_params)\n    \n#   feature elinmation \n\n#     summary = ctb_model.select_features(\n#         df_offline_train[feature_name], df_offline_train_target,\n#         eval_set=[(df_offline_valid[feature_name], df_offline_valid_target)],\n#         features_for_select=feature_name,\n#         num_features_to_select=len(feature_name)-24,    # Dropping from 124 to 100\n#         steps=3,\n#         algorithm=ctb.EFeaturesSelectionAlgorithm.RecursiveByShapValues,\n#         shap_calc_type=ctb.EShapCalcType.Regular,\n#         train_final_model=False,\n#         plot=True,\n#     )\n    ctb_model.fit(\n        df_offline_train[feature_name].astype(np.float32), df_offline_train_target.astype(np.float32),\n        eval_set=[(df_offline_valid[feature_name].astype(np.float32), df_offline_valid_target.astype(np.float32))],\n        use_best_model=True,\n#         early_stopping_rounds=200\n    )\n    \n    best_iteration_ = ctb_model.best_iteration_\n    \n    # Free up memory by deleting variables\n    del df_offline_train, df_offline_valid, df_offline_train_target, df_offline_valid_target, offline_split, ctb_model\n    gc.collect()\n\n    # Inference\n    df_train_target = targets\n    print(\"Infer Model Training.\")\n\n    # Adjust the number of estimators for the inference model\n    infer_params = ctb_params.copy()\n    infer_params[\"iterations\"] = int(1.2 * best_iteration_)\n    infer_ctb_model = ctb.CatBoostRegressor(**infer_params)\n    infer_ctb_model.fit(df_train_feats[feature_name].astype(np.float32), df_train_target.astype(np.float32))\n    return infer_ctb_model","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:19:06.533199Z","iopub.execute_input":"2023-12-12T17:19:06.533474Z","iopub.status.idle":"2023-12-12T17:19:06.546761Z","shell.execute_reply.started":"2023-12-12T17:19:06.53345Z","shell.execute_reply":"2023-12-12T17:19:06.545848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# –õ–æ–≥–∏—á–µ—Å–∫–∏–π –≤—ã–≤–æ–¥ –∏ –æ–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏–µ\n\n–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ –¥–∞—Ç–µ_id 481, 481+15, 481+30 –≤–æ –≤—Ä–µ–º—è –ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –≤—ã–≤–æ–¥–∞","metadata":{}},{"cell_type":"code","source":"def zero_sum(prices, volumes):\n    # –†–∞—Å—á–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π –æ—à–∏–±–∫–∏ –¥–ª—è –æ–±—ä–µ–º–æ–≤\n    std_error = np.sqrt(volumes)\n    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —à–∞–≥–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—É–º–º—ã —Ü–µ–Ω –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –æ—à–∏–±–æ–∫\n    step = np.sum(prices)/np.sum(std_error)\n    # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ —Ü–µ–Ω, —É—á–∏—Ç—ã–≤–∞—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –æ—à–∏–±–∫—É –∏ —à–∞–≥\n    out = prices-std_error*step\n    \n    return out\n\nimport optiver2023\nenv = optiver2023.make_env()\niter_test = env.iter_test()\ncounter = 0\ny_min, y_max = -64, 64\nqps, predictions = [], []\ncache = pd.DataFrame()\n# train_cache = pd.DataFrame()\nprint(\"–ù–∞—á–∞–ª–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏...\")\ntargets = df_train[\"target\"].astype(np.float32)\n# –£–º–µ–Ω—å—à–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\ndata = reduce_mem_usage(generate_all_features(df_train))\ndel df_train\ngc.collect()\nprint(\"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞.\")\nstart_date_id = 481  # –ù–∞—á–∞–ª—å–Ω—ã–π date_id\nstart_pro_date_id = 480\n# –û–Ω–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏–µ\ntrain_date_id = set([start_date_id, start_date_id + 15, start_date_id + 30])  # –ù–∞–±–æ—Ä date_id –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\nfeature_name = list(data.columns)\nfeature_name.remove(\"date_id\")\n# –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è CatBoost\nfeature_name_ctb = [feat for feat in feature_name if feat not in eliminated_features_names]\nfeat_last = None\ntarget_last = None\n# –°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π\ntarget_mean = targets.values.mean()\nprint(\"–ù–∞—á–∞–ª–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è...\")\nfor (test, revealed_targets, sample_prediction) in iter_test:\n    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ç–µ–∫—É—â—É—é –æ—Ü–µ–Ω–∫—É\n    currently_scored = test.iloc[0][\"currently_scored\"]\n    date_id = test.iloc[0][\"date_id\"]\n    if not currently_scored and date_id != start_pro_date_id:\n        sample_prediction['target'] = 0\n        env.predict(sample_prediction)\n        continue\n    del test['currently_scored']\n    seconds_in_bucket = test.iloc[0][\"seconds_in_bucket\"]\n    # –û–Ω–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏–µ, –µ—Å–ª–∏ —ç—Ç–æ –Ω–∞—á–∞–ª–æ –Ω–æ–≤–æ–≥–æ —Ç–æ—Ä–≥–æ–≤–æ–≥–æ –ø–µ—Ä–∏–æ–¥–∞\n    if seconds_in_bucket == 0:\n        cache = pd.DataFrame()\n        target_last = revealed_targets[\"revealed_target\"].values.astype(np.float32)\n        #concat\n        if len(targets) < len(data):\n            targets = pd.concat([targets, revealed_targets[\"revealed_target\"].astype(np.float32)], ignore_index=True, axis=0)\n\n        if date_id in train_date_id:\n            infer_lgb_model = train_lgb(data, targets, date_id - 1)\n            infer_ctb_model = train_ctb(data, targets, date_id - 1)\n            target_mean = targets.values.mean\n\n    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞\n    cache = pd.concat([cache, test], ignore_index=True, axis=0)\n    feat = generate_all_features(cache, feat_last, target_last)\n    if seconds_in_bucket == 540:\n        # –ó–∞–ø–æ–º–∏–Ω–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ü–∏–∫–ª–∞\n        feat_last = feat.copy()\n        if currently_scored:\n            # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–Ω–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è\n            data = pd.concat([data, reduce_mem_usage(feat)], ignore_index=True, axis=0)\n    if not currently_scored:\n        sample_prediction['target'] = 0\n        env.predict(sample_prediction)\n        continue\n    # –û–±—Ä–µ–∑–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞\n    feat = feat[-len(test):]\n    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–µ–π\n    lgb_prediction = infer_lgb_model.predict(feat[feature_name])\n    ctb_prediction = infer_ctb_model.predict(feat[feature_name_ctb])\n    prediction = lgb_prediction * 0.67 + ctb_prediction * 0.33\n    prediction = prediction - prediction.mean() + target_mean\n    clipped_predictions = np.clip(prediction, y_min, y_max)\n    sample_prediction['target'] = clipped_predictions\n    env.predict(sample_prediction)\nprint(\"OK\")\n","metadata":{},"execution_count":null,"outputs":[]}]}