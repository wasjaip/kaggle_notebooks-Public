{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":57891,"databundleVersionId":7056235,"sourceType":"competition"}],"dockerImageVersionId":30554,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Один из вариант решение задаи по Optiver - Trading at the Close с хорошим показателем Mean Absolute Error (MAE) ","metadata":{}},{"cell_type":"markdown","source":"# Введение\nЭтот блокнот  с нашей ансамблевой моделью LGBM и CatBoost с использованием метода отложенной кросс-валидации (holdout CV) и демонстрации процесса онлайн-обучения для непрерывного сбора данных общественного рейтинга и обновления модели (примерно через 40 дней после date_id 480).\n\n\nОсобая благодарность авторам следующих общедоступных блокнотов за их неоценимый вклад:\n\n@lblhandsome, \"⚡Optiver 🚀Robust Best ⚡ Single Model\"\n\n@zulqarnainali, \"LGB Fine-tuned 🚀(Explained)\"\n\n@verracodeguacas, \"🖐️-Fold CV 🚀\"\n\n@yekenot, \"Feature Elimination by CatBoost\"\n\n@jirkaborovec, \"📈Optiver📉: Feature Eng & Optuna🌪️LightGBM@GPU\"\n\n@yunchonggan,\"Weights of the Synthetic Index\"\n\nОбратите внимание, что для запуска следует использовать GPU T4x2. GPU P100 может вызвать проблемы с памятью при обучении Catboost на платформе Kaggle. Подробности см. в этом блокноте https://www.kaggle.com/code/yekenot/feature-elimination-by-catboost\n","metadata":{}},{"cell_type":"markdown","source":"# libraries","metadata":{}},{"cell_type":"code","source":"import gc  # Управление сборкой мусора для управления памятью\nimport os  # Функции, связанные с операционной системой\nimport time  # Функции, связанные со временем\nimport warnings  # Обработка предупреждений\nfrom itertools import combinations  # Для создания комбинаций элементов\nfrom warnings import simplefilter  # Упрощение обработки предупреждений\n\n#  Импорт библиотек машинного обучения\nimport joblib  # Для сохранения и загрузки моделей\nimport xgboost as xgb\nimport lightgbm as lgb  # Фреймворк LightGBM для градиентного бустинга\nimport catboost as ctb\nimport numpy as np  # Численные операции\nimport pandas as pd  # Манипуляция и анализ данных\n# from pandarallel import pandarallel\nfrom sklearn.metrics import mean_absolute_error  # Метрика для оценки\nfrom sklearn.model_selection import KFold, TimeSeriesSplit  # Техники кросс-валидации\n\n# Отключение предупреждений для чистоты кода\nwarnings.filterwarnings(\"ignore\")\nsimplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n# pandarallel.initialize(nb_workers=4)\n\n\nmax_lookback = np.nan  # Максимальный размер окна возврата (не указан)\nsplit_day = 435  # День разделения для данных временных рядов\nN_STOCKS = 200  # Количество акций\nMAX_N_NEIGHBOURS = 10  # Максимальное количество соседей\nNEIGHBOUR_CORR_THRESHOLD = 0.3","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:18:39.075005Z","iopub.execute_input":"2023-12-12T17:18:39.075904Z","iopub.status.idle":"2023-12-12T17:18:39.084333Z","shell.execute_reply.started":"2023-12-12T17:18:39.075868Z","shell.execute_reply":"2023-12-12T17:18:39.083255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# is_offline = True  # Флаг для режима онлайн/оффлайн\n# is_train = False  # Флаг для режима обучения\n# is_infer = False  # Флаг для режима вывода","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA ","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/optiver-trading-at-the-close/train.csv\")\n\n# Удаление строк с отсутствующими значениями в столбце \"target\"\ndf = df.dropna(subset=[\"target\"])\n# Удаляем строки, где в столбце \"target\" есть пропущенные значения.\n\ndf = df.dropna(subset=[\"wap\"])\n# Удаляем строки, где в столбце \"wap\" есть пропущенные значения.\n\n# Сброс индекса DataFrame и применение изменений на месте\ndf.reset_index(drop=True, inplace=True)\n# Сбрасываем индекс, чтобы он шел от 0 до количества строк - 1, и делаем это изменение в самом DataFrame (inplace=True).\n\n# Получение размера DataFrame (количество строк и столбцов)\ndf_shape = df.shape\n# Сохраняем размер DataFrame (количество строк и столбцов) в переменной df_shape.\n\ndf_train = df\n# Сохраняем обработанный DataFrame в переменной df_train для дальнейшего использования.\n","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:18:39.099104Z","iopub.execute_input":"2023-12-12T17:18:39.099508Z","iopub.status.idle":"2023-12-12T17:18:58.440409Z","shell.execute_reply.started":"2023-12-12T17:18:39.099468Z","shell.execute_reply":"2023-12-12T17:18:58.439504Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Оптимизация","metadata":{}},{"cell_type":"code","source":"# Функция для уменьшения использования памяти DataFrame Pandas\ndef reduce_mem_usage(df, verbose=0):\n    \"\"\"\n    Проходит через все числовые столбцы DataFrame и изменяет их тип данных\n    для уменьшения использования памяти.\n    \"\"\"\n    \n    # Расчет начального использования памяти DataFrame\n    start_mem = df.memory_usage().sum() / 1024**2\n\n    # Проход по каждому столбцу в DataFrame\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        # Проверка, не является ли тип данных столбца 'object' (т.е., числовой)\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            \n            # Проверка, является ли тип данных столбца целочисленным\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                # Проверка, является ли тип данных столбца типом с плавающей точкой\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float32)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float32)\n\n    # Предоставление информации об оптимизации памяти, если 'verbose' истина\n    if verbose:\n        logger.info(f\"Использование памяти исходного dataframe составляет {start_mem:.2f} MB\")\n        end_mem = df.memory_usage().sum() / 1024**2\n        logger.info(f\"Использование памяти после оптимизации: {end_mem:.2f} MB\")\n        decrease = 100 * (start_mem - end_mem) / start_mem\n        logger.info(f\"Уменьшено на {decrease:.2f}%\")\n\n    # Возвращение DataFrame с оптимизированным использованием памяти\n    return df\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:18:58.442757Z","iopub.execute_input":"2023-12-12T17:18:58.443076Z","iopub.status.idle":"2023-12-12T17:18:58.458166Z","shell.execute_reply.started":"2023-12-12T17:18:58.443049Z","shell.execute_reply":"2023-12-12T17:18:58.457248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# параллельный расчет триплетного дисбаланса с помощью Numba\n\nNumba - это библиотека, предназначенная для ускорения выполнения кода на языке Python путем компиляции функций в машинный код. Она часто используется для улучшения производительности при работе с числовыми вычислениями.\n\nЧтобы провести параллельные вычисления с использованием Numba, вы можете воспользоваться декоратором @njit (для Just-In-Time компиляции) и указать атрибут parallel=True. Вот пример, как это может выглядеть:\n\npython\n'''\n\n        import numpy as np\n        from numba import njit, prange\n        \n    @njit(parallel=True)\n    def calculate_triplet_imbalance(data):\n        num_elements = data.shape[0]\n        result = np.zeros(num_elements)\n\n    for i in prange(num_elements):\n        # Ваш код для вычисления триплетного дисбаланса\n        # Используйте data[i] для доступа к элементам массива\n\n    return result\n\n    # Пример использования\n    data_array = np.random.rand(1000, 3)  # Замените на ваш массив данных\n    result_array = calculate_triplet_imbalance(data_array)\n'''","metadata":{}},{"cell_type":"code","source":"# Импорт библиотеки Numba для компиляции \"на лету\" (JIT) и параллельной обработки\nfrom numba import njit, prange\n\n# Функция для вычисления дисбаланса тройных комбинаций в параллельном режиме с использованием Numba\n@njit(parallel=True)\ndef compute_triplet_imbalance(df_values, comb_indices):\n    num_rows = df_values.shape[0]\n    num_combinations = len(comb_indices)\n    imbalance_features = np.empty((num_rows, num_combinations))\n\n    # Цикл по всем комбинациям из трех элементов\n    for i in prange(num_combinations):\n        a, b, c = comb_indices[i]\n        \n        # Цикл по строкам DataFrame\n        for j in range(num_rows):\n            # Находим максимальное, минимальное и среднее значение из трех значений\n            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n            \n            # Предотвращение деления на ноль\n            if mid_val == min_val:\n                imbalance_features[j, i] = np.nan\n            else:\n                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n\n    return imbalance_features","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 📊 Function to compute triplet imbalance in parallel using Numba\n@njit(parallel=True)\ndef compute_triplet_imbalance(df_values, comb_indices):\n    num_rows = df_values.shape[0]\n    num_combinations = len(comb_indices)\n    imbalance_features = np.empty((num_rows, num_combinations))\n\n    # 🔁 Loop through all combinations of triplets\n    for i in prange(num_combinations):\n        a, b, c = comb_indices[i]\n        \n        # 🔁 Loop through rows of the DataFrame\n        for j in range(num_rows):\n\n            if df_values[j, a] < df_values[j, b]:\n                min_val = df_values[j, a]\n                max_val = df_values[j, b]\n            else:\n                max_val = df_values[j, a]\n                min_val = df_values[j, b]\n\n            if min_val < df_values[j, c]:\n                if df_values[j, c] < max_val:\n                    mid_val = df_values[j, c]\n                else:\n                    mid_val = max_val\n                    max_val = df_values[j, c]\n            else:\n                mid_val = min_val\n                min_val = df_values[j, c]\n            \n            # 🚫 Prevent division by zero\n            if max_val == min_val:\n                imbalance_features[j, i] = np.nan\n            elif mid_val == min_val:\n                imbalance_features[j, i] = np.nan\n            else:\n                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n    \n    return imbalance_features","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Функция для расчета дисбаланса тройных комбинаций по данным о ценах и DataFrame\ndef calculate_triplet_imbalance_numba(price, df):\n    # Преобразование DataFrame в массив numpy для совместимости с Numba\n    df_values = df[price].values\n    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n\n    # Вычисление дисбаланса тройных комбинаций с использованием оптимизированной функции Numba\n    features_array = compute_triplet_imbalance(df_values, comb_indices)\n\n    # Создание DataFrame из результатов\n    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n    features = pd.DataFrame(features_array, columns=columns)\n\n    return features","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# новые признаки ","metadata":{}},{"cell_type":"code","source":"# Функция для генерации признаков дисбаланса\ndef imbalance_features(df):\n\n    stock_groups = df.groupby([\"date_id\", \"seconds_in_bucket\"])\n    # Индексированный WAP\n    df[\"wwap\"] = df.stock_id.map(weights) * df.wap\n    df[\"iwap\"] = stock_groups[\"wwap\"].transform(lambda x: x.sum())\n    del df[\"wwap\"]\n\n    # Определение списков названий столбцов, связанных с ценами и размерами\n    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n\n    # V1 признаки\n    # Расчет различных признаков с использованием функции Pandas eval\n    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n    df[\"mid_price\"] = df.eval(\"(ask_price + bid_price) / 2\")\n    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n    df[\"matched_imbalance\"] = df.eval(\"(imbalance_size-matched_size)/(matched_size+imbalance_size)\")\n    df[\"all_size\"] = df.eval(\"matched_size + imbalance_size\") # добавление\n    df[\"imbalance_size_for_buy_sell\"] = df.eval(\"imbalance_size * imbalance_buy_sell_flag\")  # добавление\n    \n    cols = ['wap', 'imbalance_size_for_buy_sell', \"bid_size\", \"ask_size\"]\n    for q in [0.25, 0.5, 0.75]:  # Испытание большего/разного количества q\n        df[[f'{col}_quantile_{q}' for col in cols]] = stock_groups[cols].transform(lambda x: x.quantile(q)).astype(np.float32)\n    \n    # Создание признаков для попарных дисбалансов цен\n    for c in combinations(prices, 2):\n        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\").astype(np.float32)\n\n    for c in combinations(sizes, 2):\n        df[f\"{c[0]}/{c[1]}\"] = df.eval(f\"({c[0]})/({c[1]})\").astype(np.float32)\n\n    # Расчет признаков дисбаланса тройных комбинаций с использованием оптимизированной функции Numba\n    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n        df[triplet_feature.columns] = triplet_feature.values.astype(np.float32)\n        \n    # V2 признаки\n    # Расчет дополнительных признаков\n    stock_groups = df.groupby(['stock_id', 'date_id'])\n    df[\"imbalance_momentum\"] = stock_groups['imbalance_size'].diff(periods=1) / df['matched_size']\n    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n    df[\"spread_intensity\"] = stock_groups['price_spread'].diff()\n    df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n    df['wap_advantage'] = df.wap - df.iwap  # добавление\n\n    # Расчет различных статистических агрегированных признаков\n    df_prices = df[prices]\n    df_sizes = df[sizes]\n    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n        df[f\"all_prices_{func}\"] = df_prices.agg(func, axis=1)\n        df[f\"all_sizes_{func}\"] = df_sizes.agg(func, axis=1)\n        \n    # V3 признаки\n    # Расчет сдвинутых и возвращаемых признаков для конкретных столбцов\n    cols = ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag', \"wap\", \"iwap\"]\n    stock_groups_cols = stock_groups[cols]\n    for window in [1, 2, 3, 6, 10]:\n        df[[f\"{col}_shift_{window}\" for col in cols]] = stock_groups_cols.shift(window)\n\n    cols = ['matched_size', 'imbalance_size', 'reference_price', \"iwap\"] #wap\n    stock_groups_cols = stock_groups[cols]\n    for window in [1, 2, 3, 6, 10]:\n        df[[f\"{col}_ret_{window}\" for col in cols]] = stock_groups_cols.pct_change(window).astype(np.float32)\n\n    # Расчет признаков разницы для конкретных столбцов\n    cols = ['ask_price', 'bid_price', 'ask_size', 'bid_size', 'wap', 'near_price', 'far_price', 'imbalance_size_for_buy_sell']\n    stock_groups_cols = stock_groups[cols]\n    for window in [1, 2, 3, 6, 10]:\n        df[[f\"{col}_diff_{window}\" for col in cols]] = stock_groups_cols.diff(window).astype(np.float32)\n\n    # V4 признаки\n    # Построение признака `time_since_last_imbalance_change`\n    df['flag_change'] = stock_groups['imbalance_buy_sell_flag'].diff().ne(0).astype(int)\n    # Использование cumsum для создания идентификатора группы, увеличивающегося при каждом изменении флага\n    df['group'] = df.groupby(['stock_id', 'date_id'])['flag_change'].cumsum()\n    # Расчет временного интервала с момента последнего изменения флага в каждой группе\n    group_min = df.groupby(['stock_id', 'date_id', 'group'])['seconds_in_bucket'].transform('min')\n    df['time_since_last_imbalance_change'] = df['seconds_in_bucket'] - group_min\n    # Обнуление времени в местах, где произошло изменение флага\n    df['time_since_last_imbalance_change'] *= (1 - df['flag_change'])\n    df.drop(columns=['flag_change', 'group'], inplace=True)\n    \n    cols = ['imbalance_size_for_buy_sell']\n    stock_groups_cols = stock_groups[cols]\n    for window in [5, 10]:\n        mean_col = stock_groups_cols.transform(lambda x: x.rolling(window=window).mean())\n        std_col = stock_groups_cols.transform(lambda x: x.rolling(window=window).std())\n        df[[f'z_score_{col}_{window}' for col in cols]] = (df[cols] - mean_col) / std_col\n    \n    # Замена бесконечных значений на 0\n    return df.replace([np.inf, -np.inf], 0)\n\n# Функция для генерации временных и акционных признаков\ndef other_features(df):\n    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60  # Секунды\n    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60  # Минуты\n\n    # Присвоение глобальных признаков к DataFrame\n    for key, value in global_stock_id_feats.items():\n        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n        \n    for key, value in global_seconds_feats.items():\n        df[f\"global_seconds_{key}\"] = df[\"seconds_in_bucket\"].map(value.to_dict())\n\n    return df\n\n           \n","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:18:59.240287Z","iopub.execute_input":"2023-12-12T17:18:59.240593Z","iopub.status.idle":"2023-12-12T17:18:59.28331Z","shell.execute_reply.started":"2023-12-12T17:18:59.240566Z","shell.execute_reply":"2023-12-12T17:18:59.282367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"добавление признаков последних дней, включая подготовку данных, сдвиг столбцов для создания признаков и объединение различных наборов признаков. В функции generate_all_features применяются различные методы для создания комплексного набора признаков, используемого в анализе данных.","metadata":{}},{"cell_type":"code","source":"def last_days_features(df: pd.DataFrame, feat_last=None, target_last=None):\n    size = None\n    \n    # Если предыдущие признаки предоставлены и их количество больше нуля\n    if feat_last is not None and len(feat_last) > 0:\n        # Выбор столбцов, которые присутствуют как в текущем, так и в предыдущем DataFrame\n        cols = [col for col in df.columns if col in set(feat_last.columns)]\n        # Если предоставлены целевые значения для предыдущих признаков\n        if target_last is not None:\n            # Добавление целевого столбца и заполнение его значениями\n            cols.append(\"target\")\n            feat_last[\"target\"] = target_last\n            df[\"target\"] = 0\n        # Подготовка к добавлению заполнителей\n        paddings = []\n        # Нахождение максимального значения секунд в текущем DataFrame\n        second_start = df.seconds_in_bucket.max()\n        # Получение источника данных для заполнителей\n        padding_src = df[df.seconds_in_bucket == second_start]\n        # Определение размеров текущего и заполнительного DataFrame\n        size = len(df)\n        size_pad = len(padding_src) * 6\n        # Добавление заполнителей для каждого шага в 10 секунд\n        for second in range(second_start + 10, second_start + 70, 10):\n            padding = padding_src.copy()\n            padding[\"seconds_in_bucket\"] = second\n            paddings.append(padding)\n        # Объединение предыдущих признаков, текущих данных и заполнителей\n        df = pd.concat([feat_last[cols], df] + paddings)\n\n    # Добавление признаков последних дней\n    # TODO: Попробовать больше признаков\n    cols = ['near_price', 'far_price', 'depth_pressure']\n    if 'target' in df.columns:\n        cols.append('target')\n    stock_groups = df.groupby(['stock_id', 'seconds_in_bucket'])\n    stock_groups_cols = stock_groups[cols]\n    # Сдвиг столбцов для создания признаков\n    for window in [1]:\n        df[[f\"{col}_last_{window}day\" for col in cols]] = stock_groups_cols.shift(window)\n    if cols[-1] == \"target\":\n        cols.pop()\n    \n    cols = [f\"{col}_last_{window}day\" for col in cols]\n    stock_groups = df.groupby(['stock_id', 'date_id'])\n    stock_groups_cols = stock_groups[cols]\n    # Сдвиг столбцов для создания будущих признаков\n    for window in [1, 2, 3, 6]:\n        df[[f\"{col}_future_{window}\" for col in cols]] = stock_groups_cols.shift(-window)\n        \n    if size:\n        return df[-(size + size_pad):-size_pad]\n    return df\n\n# Функция для генерации всех признаков путем комбинирования признаков дисбаланса и других признаков\ndef generate_all_features(df, feat_last=None, target_last=None):\n    # Выбор релевантных столбцов для генерации признаков\n    cols = [c for c in df.columns if c not in {\"row_id\", \"time_id\", \"currently_scored\"}]\n    df = df[cols]\n    \n    # Генерация признаков дисбаланса\n    df = imbalance_features(df)\n    \n    # Генерация признаков последних дней\n    df = last_days_features(df, feat_last, target_last)\n    \n    # Генерация временных и акционных признаков\n    df = other_features(df)\n\n    gc.collect()  # Осуществление сборки мусора для освобождения памяти\n    \n    # Выбор и возврат сгенерированных признаков\n    feature_name = [i for i in df.columns if i not in {\"row_id\", \"target\", \"time_id\"}]\n    \n    return df[feature_name]\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ВЕса ","metadata":{}},{"cell_type":"code","source":"weights = [\n    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n]\n\nweights = {int(k):v for k,v in enumerate(weights)}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"оздание глобальных признаков на основе группировки данных по идентификатору акций и по времени в секундах","metadata":{}},{"cell_type":"code","source":"# Группировка данных обучающего набора по идентификатору акций\nstock_group = df_train.groupby(\"stock_id\")\n\n# Создание нового столбца \"imbalance_size_for_buy_sell\" путем умножения размера дисбаланса на флаг покупки/продажи\ndf_train[\"imbalance_size_for_buy_sell\"] = df_train.eval(\"imbalance_size * imbalance_buy_sell_flag\")\n\n# Глобальные признаки по идентификатору акций\nglobal_stock_id_feats = {\n    \"median_size\": stock_group[\"bid_size\"].median() + stock_group[\"ask_size\"].median(),  # Медиана размера заявок покупки и продажи\n    \"std_size\": stock_group[\"bid_size\"].std() + stock_group[\"ask_size\"].std(),  # Стандартное отклонение размера заявок покупки и продажи\n    \"ptp_size\": stock_group[\"bid_size\"].max() - stock_group[\"bid_size\"].min(),  # Размах размера заявок покупки\n    \"median_price\": stock_group[\"bid_price\"].median() + stock_group[\"ask_price\"].median(),  # Медиана цен заявок покупки и продажи\n    \"std_price\": stock_group[\"bid_price\"].std() + stock_group[\"ask_price\"].std(),  # Стандартное отклонение цен заявок покупки и продажи\n    \"ptp_price\": stock_group[\"bid_price\"].max() - stock_group[\"ask_price\"].min(),  # Размах цен заявок покупки и продажи\n    \"median_far_price\": stock_group[\"far_price\"].median(),  # Медиана дальней цены\n    \"median_near_price\": stock_group[\"near_price\"].median(),  # Медиана ближней цены\n    \"median_imbalance_size_for_buy_sell\": stock_group[\"imbalance_size_for_buy_sell\"].median(),  # Медиана дисбаланса размера для покупки/продажи\n    \"matched_size\":stock_group[\"matched_size\"].median(),  # Медиана согласованного размера\n}\n\n# Глобальные признаки по секундам\nglobal_seconds_feats = {\n    # Медиана целевого значения по секундам внутри каждой даты, а затем в целом по секундам\n    \"median_target\": df_train.groupby([\"date_id\", \"seconds_in_bucket\"])[\"target\"].apply(lambda x: x.abs().mean()).reset_index(0, drop=True).groupby(\"seconds_in_bucket\").median(),\n}\n","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:18:59.284558Z","iopub.execute_input":"2023-12-12T17:18:59.284886Z","iopub.status.idle":"2023-12-12T17:19:06.484298Z","shell.execute_reply.started":"2023-12-12T17:18:59.284859Z","shell.execute_reply":"2023-12-12T17:19:06.483441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LGBM \n\n\nLGBM параметры из  **@jirkaborovec**, [\"📈Optiver📉: Feature Eng & Optuna🌪️LightGBM@GPU\"](https://www.kaggle.com/code/jirkaborovec/optiver-feature-eng-optuna-lightgbm-gpu) ","metadata":{}},{"cell_type":"code","source":"def train_lgb(df_train_feats, targets, split_day):\n\n    feature_name = list(df_train_feats.columns)\n    feature_name.remove(\"date_id\")\n    offline_split = df_train_feats['date_id'] > (split_day - 45)\n    \n\n    stock_group = df_train_feats.groupby(\"stock_id\")\n    global global_stock_id_feats, global_seconds_feats\n    global_stock_id_feats = {\n        \"median_size\": stock_group[\"bid_size\"].median() + stock_group[\"ask_size\"].median(),\n        \"std_size\": stock_group[\"bid_size\"].std() + stock_group[\"ask_size\"].std(),\n        \"ptp_size\": stock_group[\"bid_size\"].max() - stock_group[\"bid_size\"].min(),\n        \"median_price\": stock_group[\"bid_price\"].median() + stock_group[\"ask_price\"].median(),\n        \"std_price\": stock_group[\"bid_price\"].std() + stock_group[\"ask_price\"].std(),\n        \"ptp_price\": stock_group[\"bid_price\"].max() - stock_group[\"ask_price\"].min(),\n        \"median_far_price\": stock_group[\"far_price\"].median(),\n        \"median_near_price\": stock_group[\"near_price\"].median(),\n        \"median_imbalance_size_for_buy_sell\": stock_group[\"imbalance_size_for_buy_sell\"].median(),\n        \"matched_size\": stock_group[\"matched_size\"].median(),\n    }\n    df_train_feats[\"target\"] = targets.values\n    global_seconds_feats = {\n        \"median_target\": df_train_feats.groupby([\"date_id\", \"seconds_in_bucket\"])[\"target\"].apply(lambda x: x.abs().mean()).reset_index(0, drop=True).groupby(\"seconds_in_bucket\").median(),\n    }\n    del df_train_feats[\"target\"]\n    \n    # update global features\n    for key, value in global_stock_id_feats.items():\n        df_train_feats[f\"global_{key}\"] = df_train_feats[\"stock_id\"].map(value.to_dict())\n        \n    for key, value in global_seconds_feats.items():\n        df_train_feats[f\"global_seconds_{key}\"] = df_train_feats[\"seconds_in_bucket\"].map(value.to_dict())\n        \n    lgb_params = {\n        \"objective\": \"mae\",\n        \"n_estimators\": 5500,\n        \"num_leaves\": 465,\n        \"subsample\": 0.65791,\n        \"colsample_bytree\": 0.7,\n        \"learning_rate\": 0.00877,  # 0.00877\n        \"n_jobs\": 4,\n        \"device\": \"gpu\",\n        \"verbosity\": -1,\n        \"importance_type\": \"gain\",\n        \"max_depth\": 14,  # Maximum depth of the tree\n        \"min_child_samples\": 132,  # Minimum number of data points in a leaf\n        \"reg_alpha\": 6,  # L1 regularization term\n        \"reg_lambda\": 0.08,  # L2 regularization term\n    }\n\n    print(f\"Feature length = {len(feature_name)}\")\n\n    # Split data for offline training based on a specific date\n    df_offline_train = df_train_feats[~offline_split]\n    df_offline_valid = df_train_feats[offline_split]\n    df_offline_train_target = targets[~offline_split]\n    df_offline_valid_target = targets[offline_split]\n    \n    \n    train_dataset = lgb.Dataset(df_offline_train[feature_name].values.astype(np.float32), \n                                label=df_offline_train_target.values.astype(np.float32))\n    \n    valid_dataset = lgb.Dataset(df_offline_valid[feature_name].values.astype(np.float32), \n                                label=df_offline_valid_target.values.astype(np.float32))\n    \n    del df_offline_train, df_offline_valid, df_offline_train_target, df_offline_valid_target, offline_split\n\n    print(\"Valid Model Training.\")\n    \n    # Train a LightGBM model on the offline data\n    lgb_model = lgb.LGBMRegressor(**lgb_params)\n  \n    lgb_model.fit(\n        train_dataset.data,\n        train_dataset.label,\n        eval_set=[(valid_dataset.data, valid_dataset.label)],\n        callbacks=[\n            lgb.callback.early_stopping(stopping_rounds=200),\n            lgb.callback.log_evaluation(period=100),\n        ],\n        feature_name=feature_name,\n    )\n    \n    best_iteration_ = lgb_model.best_iteration_\n    \n    # Free up memory by deleting variables\n    del lgb_model\n    gc.collect()\n\n    # Inference\n    df_train_target = targets\n    full_train_dataset = lgb.Dataset(df_train_feats[feature_name].values.astype(np.float32), \n                                     label=df_train_target.values.astype(np.float32))\n    print(\"Infer Model Training.\")\n\n    # Adjust the number of estimators for the inference model\n    infer_params = lgb_params.copy()\n    infer_params[\"n_estimators\"] = int(1.2 * best_iteration_)\n    infer_lgb_model = lgb.LGBMRegressor(**infer_params)\n    \n    infer_lgb_model.fit(\n        full_train_dataset.data,\n        full_train_dataset.label,\n        feature_name=feature_name,\n        )\n    \n#     infer_lgb_model.fit(\n#         df_train_feats[feature_name].values.astype(np.float32), \n#         df_train_target.values.astype(np.float32), \n#         feature_name = feature_name\n#         )\n\n    return infer_lgb_model","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:19:06.49617Z","iopub.execute_input":"2023-12-12T17:19:06.496502Z","iopub.status.idle":"2023-12-12T17:19:06.518112Z","shell.execute_reply.started":"2023-12-12T17:19:06.496475Z","shell.execute_reply":"2023-12-12T17:19:06.517162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Catboost тренировка \n\nСпасибо  **@yekenot**, [\"Feature Elimination by CatBoost\"](https://www.kaggle.com/code/yekenot/feature-elimination-by-catboost)\n","metadata":{}},{"cell_type":"code","source":"eliminated_features_names = set(['near_price_ask_price_imb',\n                             'iwap',\n                             'all_imbalance_size_for_buy_sell_quantile_0.5',\n                             'iwap_shift_10',\n                             'all_imbalance_size_for_buy_sell_quantile_0.75',\n                             'matched_size_ret_10',\n                             'all_wap_quantile_0.25',\n                             'iwap_shift_6',\n                             'near_price_wap_imb',\n                             'iwap_shift_1',\n                             'iwap_ret_10',\n                             'iwap_ret_6',\n                             'all_imbalance_size_for_buy_sell_quantile_0.25',\n                             'global_ptp_size',\n                             'reference_price_shift_10',\n                             'all_ask_size_quantile_0.75',\n                             'iwap_shift_3',\n                             'iwap_shift_2',\n                             'reference_price_near_price_imb',\n                             'all_bid_size_quantile_0.25',\n                             'global_std_size',\n                             'global_median_price',\n                             'matched_size_ret_6',\n                             'wap_shift_10'])","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:19:06.519207Z","iopub.execute_input":"2023-12-12T17:19:06.519474Z","iopub.status.idle":"2023-12-12T17:19:06.532037Z","shell.execute_reply.started":"2023-12-12T17:19:06.51945Z","shell.execute_reply":"2023-12-12T17:19:06.531179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_ctb(df_train_feats, targets, split_day):\n\n    feature_name = list(df_train_feats.columns)\n    feature_name.remove(\"date_id\")\n    feature_name = [feat for feat in feature_name if feat not in eliminated_features_names]\n    offline_split = df_train_feats['date_id'] > (split_day - 45)\n    \n#     target_mean = targets.values.mean()\n#     pre = df_train_feats[offline_split][[\"date_id\", \"seconds_in_bucket\"]]\n#     pre[\"target\"] = targets[offline_split].values\n    \n    ctb_params = dict(iterations=2000,\n                      learning_rate=1.0,\n                      depth=9,\n                      l2_leaf_reg=30,\n                      bootstrap_type='Bernoulli',\n                      subsample=0.66,\n                      loss_function='MAE',\n                      eval_metric = 'MAE',\n                      metric_period=100,\n                      od_type='Iter',\n                      od_wait=200,\n                      task_type='GPU',\n                      allow_writing_files=False,\n                      random_strength=4.428571428571429\n                      )\n\n    print(f\"Feature length = {len(feature_name)}\")\n\n    # Split data for offline training based on a specific date\n    df_offline_train = df_train_feats[~offline_split]\n    df_offline_valid = df_train_feats[offline_split]\n    df_offline_train_target = targets[~offline_split]\n    df_offline_valid_target = targets[offline_split]\n    \n#     train_dataset = lgb.Dataset(df_offline_train[feature_name].values, label = df_offline_train_target, feature_name = feature_name)\n#     valid_dataset = lgb.Dataset(df_offline_valid[feature_name].values, label = df_offline_valid_target, feature_name = feature_name)\n\n    print(\"Valid Model Training.\")\n\n    # Train a LightGBM model on the offline data\n    ctb_model = ctb.CatBoostRegressor(**ctb_params)\n    \n#   feature elinmation \n\n#     summary = ctb_model.select_features(\n#         df_offline_train[feature_name], df_offline_train_target,\n#         eval_set=[(df_offline_valid[feature_name], df_offline_valid_target)],\n#         features_for_select=feature_name,\n#         num_features_to_select=len(feature_name)-24,    # Dropping from 124 to 100\n#         steps=3,\n#         algorithm=ctb.EFeaturesSelectionAlgorithm.RecursiveByShapValues,\n#         shap_calc_type=ctb.EShapCalcType.Regular,\n#         train_final_model=False,\n#         plot=True,\n#     )\n    ctb_model.fit(\n        df_offline_train[feature_name].astype(np.float32), df_offline_train_target.astype(np.float32),\n        eval_set=[(df_offline_valid[feature_name].astype(np.float32), df_offline_valid_target.astype(np.float32))],\n        use_best_model=True,\n#         early_stopping_rounds=200\n    )\n    \n    best_iteration_ = ctb_model.best_iteration_\n    \n    # Free up memory by deleting variables\n    del df_offline_train, df_offline_valid, df_offline_train_target, df_offline_valid_target, offline_split, ctb_model\n    gc.collect()\n\n    # Inference\n    df_train_target = targets\n    print(\"Infer Model Training.\")\n\n    # Adjust the number of estimators for the inference model\n    infer_params = ctb_params.copy()\n    infer_params[\"iterations\"] = int(1.2 * best_iteration_)\n    infer_ctb_model = ctb.CatBoostRegressor(**infer_params)\n    infer_ctb_model.fit(df_train_feats[feature_name].astype(np.float32), df_train_target.astype(np.float32))\n    return infer_ctb_model","metadata":{"execution":{"iopub.status.busy":"2023-12-12T17:19:06.533199Z","iopub.execute_input":"2023-12-12T17:19:06.533474Z","iopub.status.idle":"2023-12-12T17:19:06.546761Z","shell.execute_reply.started":"2023-12-12T17:19:06.53345Z","shell.execute_reply":"2023-12-12T17:19:06.545848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Логический вывод и онлайн-обучение\n\nОбучение модели по дате_id 481, 481+15, 481+30 во время логического вывода","metadata":{}},{"cell_type":"code","source":"def zero_sum(prices, volumes):\n    # Расчет стандартной ошибки для объемов\n    std_error = np.sqrt(volumes)\n    # Вычисление шага на основе суммы цен и стандартных ошибок\n    step = np.sum(prices)/np.sum(std_error)\n    # Корректировка цен, учитывая стандартную ошибку и шаг\n    out = prices-std_error*step\n    \n    return out\n\nimport optiver2023\nenv = optiver2023.make_env()\niter_test = env.iter_test()\ncounter = 0\ny_min, y_max = -64, 64\nqps, predictions = [], []\ncache = pd.DataFrame()\n# train_cache = pd.DataFrame()\nprint(\"Начало инициализации...\")\ntargets = df_train[\"target\"].astype(np.float32)\n# Уменьшение использования памяти и генерация всех признаков\ndata = reduce_mem_usage(generate_all_features(df_train))\ndel df_train\ngc.collect()\nprint(\"Инициализация завершена.\")\nstart_date_id = 481  # Начальный date_id\nstart_pro_date_id = 480\n# Онлайн обучение\ntrain_date_id = set([start_date_id, start_date_id + 15, start_date_id + 30])  # Набор date_id для обучения\nfeature_name = list(data.columns)\nfeature_name.remove(\"date_id\")\n# Отфильтрованные признаки для CatBoost\nfeature_name_ctb = [feat for feat in feature_name if feat not in eliminated_features_names]\nfeat_last = None\ntarget_last = None\n# Среднее значение целевой переменной\ntarget_mean = targets.values.mean()\nprint(\"Начало предсказания...\")\nfor (test, revealed_targets, sample_prediction) in iter_test:\n    # Проверка на текущую оценку\n    currently_scored = test.iloc[0][\"currently_scored\"]\n    date_id = test.iloc[0][\"date_id\"]\n    if not currently_scored and date_id != start_pro_date_id:\n        sample_prediction['target'] = 0\n        env.predict(sample_prediction)\n        continue\n    del test['currently_scored']\n    seconds_in_bucket = test.iloc[0][\"seconds_in_bucket\"]\n    # Онлайн обучение, если это начало нового торгового периода\n    if seconds_in_bucket == 0:\n        cache = pd.DataFrame()\n        target_last = revealed_targets[\"revealed_target\"].values.astype(np.float32)\n        #concat\n        if len(targets) < len(data):\n            targets = pd.concat([targets, revealed_targets[\"revealed_target\"].astype(np.float32)], ignore_index=True, axis=0)\n\n        if date_id in train_date_id:\n            infer_lgb_model = train_lgb(data, targets, date_id - 1)\n            infer_ctb_model = train_ctb(data, targets, date_id - 1)\n            target_mean = targets.values.mean\n\n    # Обработка данных тестового набора\n    cache = pd.concat([cache, test], ignore_index=True, axis=0)\n    feat = generate_all_features(cache, feat_last, target_last)\n    if seconds_in_bucket == 540:\n        # Запоминание последних признаков для следующего цикла\n        feat_last = feat.copy()\n        if currently_scored:\n            # Объединение данных для онлайн обучения\n            data = pd.concat([data, reduce_mem_usage(feat)], ignore_index=True, axis=0)\n    if not currently_scored:\n        sample_prediction['target'] = 0\n        env.predict(sample_prediction)\n        continue\n    # Обрезание признаков для текущего тестового набора\n    feat = feat[-len(test):]\n    # Предсказание с использованием моделей\n    lgb_prediction = infer_lgb_model.predict(feat[feature_name])\n    ctb_prediction = infer_ctb_model.predict(feat[feature_name_ctb])\n    prediction = lgb_prediction * 0.67 + ctb_prediction * 0.33\n    prediction = prediction - prediction.mean() + target_mean\n    clipped_predictions = np.clip(prediction, y_min, y_max)\n    sample_prediction['target'] = clipped_predictions\n    env.predict(sample_prediction)\nprint(\"OK\")\n","metadata":{},"execution_count":null,"outputs":[]}]}