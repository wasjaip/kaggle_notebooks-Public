{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":57236,"databundleVersionId":7292407,"sourceType":"competition"},{"sourceId":7223258,"sourceType":"datasetVersion","datasetId":4181108}],"dockerImageVersionId":30626,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### The feature engineering functions and the baseline are taken from [this notebook](https://www.kaggle.com/code/ahmedabdulwahab/pandas-data-description-and-starters-guide)","metadata":{}},{"cell_type":"markdown","source":"## Importing Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime, date, timedelta\nimport enefit\nfrom sklearn.metrics import mean_absolute_error\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import VotingRegressor\n\nimport warnings\nwarnings.filterwarnings('ignore')\n","metadata":{"execution":{"iopub.status.busy":"2023-12-26T22:11:37.406362Z","iopub.execute_input":"2023-12-26T22:11:37.406825Z","iopub.status.idle":"2023-12-26T22:11:37.412968Z","shell.execute_reply.started":"2023-12-26T22:11:37.406789Z","shell.execute_reply":"2023-12-26T22:11:37.411966Z"},"trusted":true},"execution_count":161,"outputs":[]},{"cell_type":"code","source":"train= pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/train.csv')\ngas_df= pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/gas_prices.csv')\nelectricity_df= pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/electricity_prices.csv')\nclient_df= pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/client.csv')\nfw_df= pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/forecast_weather.csv')\nhw_df= pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/historical_weather.csv')\nlocations= pd.read_csv('/kaggle/input/locations/county_lon_lats.csv')","metadata":{"execution":{"iopub.status.busy":"2023-12-26T22:11:37.417876Z","iopub.execute_input":"2023-12-26T22:11:37.418144Z","iopub.status.idle":"2023-12-26T22:11:52.782234Z","shell.execute_reply.started":"2023-12-26T22:11:37.418121Z","shell.execute_reply":"2023-12-26T22:11:52.781151Z"},"trusted":true},"execution_count":162,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering functions","metadata":{"execution":{"iopub.status.busy":"2023-12-16T18:58:24.479216Z","iopub.execute_input":"2023-12-16T18:58:24.479531Z","iopub.status.idle":"2023-12-16T18:58:24.646928Z","shell.execute_reply.started":"2023-12-16T18:58:24.479495Z","shell.execute_reply":"2023-12-16T18:58:24.646014Z"}}},{"cell_type":"code","source":"def feat_eng_train(data, client, hist_weather,forecast_weather, electricity, gas, locations):\n\n    data= data[data['target'].notnull()] \n    \n    data['datetime'] = pd.to_datetime(data['datetime'], utc=True)\n    \n    electricity = electricity.rename(columns= {'forecast_date' : 'datetime'})\n    \n    electricity['datetime'] = pd.to_datetime(electricity['datetime'], utc= True)\n    \n    # Decreasing (data_block_id) in client data because it's 2 steps ahead from train's data (data_block_id)\n    client['data_block_id'] -= 2\n    \n    # locations is a custom data that will help replace (latitude) and (longitude)columns by the counties for each coordination \n    locations = locations.drop('Unnamed: 0', axis= 1) \n        \n    forecast_weather[['latitude', 'longitude']] = forecast_weather[['latitude','longitude']].astype(float).round(1)\n        \n    forecast_weather= forecast_weather.merge(locations, how='left',on=['longitude','latitude'])\n        \n    forecast_weather.dropna(axis= 0, inplace= True)    \n    \n    forecast_weather['county'] = forecast_weather['county'].astype('int64')\n    \n    forecast_weather.drop(['origin_datetime', 'latitude','longitude', 'hours_ahead', 'data_block_id'], axis=1, inplace= True)\n    \n    forecast_weather.rename(columns={'forecast_datetime': 'datetime'}, inplace= True)\n    \n    forecast_weather['datetime']= pd.to_datetime(forecast_weather['datetime'], utc= True)\n    \n    # Grouping all forecast_weather columns mean values by hour, So each hour will have the mean values of the forecast_weather columns\n    forecast_weather_datetime= forecast_weather.groupby([forecast_weather['datetime'].dt.to_period('h')])[list(forecast_weather.drop(['county','datetime'], axis= 1).columns)].mean().reset_index()\n    \n    forecast_weather_datetime['datetime']= pd.to_datetime(forecast_weather_datetime['datetime'].dt.to_timestamp(), utc=True)\n    \n    # Grouping all forecast_weather columns mean values by hour and county, So each hour and county will have the mean values of the forecast_weather columns for each county\n    forecast_weather_datetime_county= forecast_weather.groupby(['county',forecast_weather['datetime'].dt.to_period('h')])[list(forecast_weather.drop(['county','datetime'], axis= 1).columns)].mean().reset_index()\n    \n    forecast_weather_datetime_county['datetime']= pd.to_datetime(forecast_weather_datetime_county['datetime'].dt.to_timestamp(), utc=True)\n           \n    hist_weather[['latitude', 'longitude']] = hist_weather[['latitude', 'longitude']].astype(float).round(1)\n\n    hist_weather= hist_weather.merge(locations, how='left', on=['longitude','latitude'])    \n    \n    hist_weather.dropna(axis= 0, inplace= True)\n    \n    hist_weather.drop(['latitude', 'longitude'], axis=1, inplace= True)\n  \n    hist_weather['county'] = hist_weather['county'].astype('int64')\n    \n    hist_weather['datetime']= pd.to_datetime(hist_weather['datetime'], utc= True)\n    \n    # Grouping all historical_weather columns mean values by hour, So each hour will have the mean values of the historical_weather columns\n    hist_weather_datetime= hist_weather.groupby([hist_weather['datetime'].dt.to_period('h')])[list(hist_weather.drop(['county','datetime','data_block_id'], axis= 1).columns)].mean().reset_index()    \n    \n   \n    hist_weather_datetime['datetime']= pd.to_datetime(hist_weather_datetime['datetime'].dt.to_timestamp(), utc=True)\n    \n    hist_weather_datetime= hist_weather_datetime.merge(hist_weather[['datetime', 'data_block_id']], how='left', on='datetime')\n    \n    #  Grouping all historical_weather columns mean values by hour and county, So each hour will have the mean values of the historical_weather columns for each county\n    hist_weather_datetime_county= hist_weather.groupby(['county',hist_weather['datetime'].dt.to_period('h')])[list(hist_weather.drop(['county','datetime', 'data_block_id'], axis= 1).columns)].mean().reset_index() \n    \n    hist_weather_datetime_county['datetime']= pd.to_datetime(hist_weather_datetime_county['datetime'].dt.to_timestamp(), utc=True)\n    \n    hist_weather_datetime_county= hist_weather_datetime_county.merge(hist_weather[['datetime', 'data_block_id']], how='left', on='datetime')\n    \n    data['year'] = data['datetime'].dt.year\n    data['month'] = data['datetime'].dt.month\n    data['day'] = data['datetime'].dt.day\n    data['hour'] = data['datetime'].dt.hour\n    data['dayofweek'] = data['datetime'].dt.dayofweek\n    data['dayofyear']= data['datetime'].dt.dayofyear\n    \n    electricity['hour'] = electricity['datetime'].dt.hour\n\n    data= data.merge(client.drop(columns = ['date']), how='left', on=['data_block_id', 'county', 'is_business', 'product_type'])\n    data= data.merge(gas[['data_block_id', 'lowest_price_per_mwh', 'highest_price_per_mwh']], how='left', on='data_block_id')\n    data= data.merge(electricity[['euros_per_mwh', 'hour', 'data_block_id']], how='left', on=['hour', 'data_block_id']) \n    data= data.merge(forecast_weather_datetime, how='left', on=['datetime'])  \n    data= data.merge(forecast_weather_datetime_county, how='left', on=['datetime', 'county'],suffixes= ('_fcast_mean','_fcast_mean_by_county'))\n    \n    hist_weather_datetime['hour']= hist_weather_datetime['datetime'].dt.hour\n    hist_weather_datetime_county['hour']= hist_weather_datetime_county['datetime'].dt.hour\n\n    hist_weather_datetime.drop_duplicates(inplace=True)\n    hist_weather_datetime_county.drop_duplicates(inplace=True)\n    hist_weather_datetime.drop('datetime', axis= 1, inplace= True)\n    hist_weather_datetime_county.drop('datetime', axis= 1, inplace= True)\n\n    data= data.merge(hist_weather_datetime, how='left', on=['data_block_id', 'hour'])\n    data= data.merge(hist_weather_datetime_county, how='left', on=['data_block_id', 'county', 'hour'],suffixes= ('_hist_mean','_hist_mean_by_county'))\n    data= data.groupby(['year', 'day', 'hour'], as_index=False).apply(lambda x: x.ffill().bfill()).reset_index()\n    data.drop(['level_0', 'level_1', 'row_id', 'data_block_id'], axis= 1, inplace= True)\n    \n    return data","metadata":{"execution":{"iopub.status.busy":"2023-12-26T22:11:52.784323Z","iopub.execute_input":"2023-12-26T22:11:52.784702Z","iopub.status.idle":"2023-12-26T22:11:52.812342Z","shell.execute_reply.started":"2023-12-26T22:11:52.784667Z","shell.execute_reply":"2023-12-26T22:11:52.811163Z"},"trusted":true},"execution_count":163,"outputs":[]},{"cell_type":"code","source":"def create_revealed_targets_train(data, N_day_lags):\n    # Creating lagged target values => A target value depends on what was a target value n days ago\n    \n    original_datetime = data['datetime']\n    \n    revealed_targets = data[['datetime', 'prediction_unit_id', 'is_consumption', 'target']].copy()\n    \n    #Creating revealed targets for 'n' days lag\n    for day_lag in range(2, N_day_lags+1):\n        revealed_targets['datetime'] = original_datetime + pd.DateOffset(day_lag)\n        data = data.merge(revealed_targets, how='left', on = ['datetime', 'prediction_unit_id', 'is_consumption'],suffixes = ('', f'_{day_lag}_days_ago'))\n    return data","metadata":{"execution":{"iopub.status.busy":"2023-12-26T22:11:52.813592Z","iopub.execute_input":"2023-12-26T22:11:52.813926Z","iopub.status.idle":"2023-12-26T22:11:52.827344Z","shell.execute_reply.started":"2023-12-26T22:11:52.813888Z","shell.execute_reply":"2023-12-26T22:11:52.826378Z"},"trusted":true},"execution_count":164,"outputs":[]},{"cell_type":"code","source":"# Применение функции предварительной обработки данных к обучающему набору. Эта функция интегрирует информацию из различных источников данных, \n# таких как данные клиентов, аппаратного и программного обеспечения, электроэнергии и газа, а также местоположений.\ntrain = feat_eng_train(train, client_df, hw_df, fw_df, electricity_df, gas_df, locations)\n\n# Создание лагов для переменных за последние N дней (в данном случае 7 дней). Это помогает модели учесть временные зависимости в данных.\nN_day_lags = 7\ntrain = create_revealed_targets_train(train, N_day_lags=N_day_lags)\n\n# Преобразование столбца 'datetime' в числовой формат. Это может быть необходимо для обработки дат и времени моделями машинного обучения,\n# которые обычно работают с числовыми данными.\ntrain['datetime'] = train['datetime'].astype('int64')\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-26T22:11:52.829466Z","iopub.execute_input":"2023-12-26T22:11:52.829782Z","iopub.status.idle":"2023-12-26T22:12:24.277765Z","shell.execute_reply.started":"2023-12-26T22:11:52.829756Z","shell.execute_reply":"2023-12-26T22:12:24.276669Z"},"trusted":true},"execution_count":165,"outputs":[]},{"cell_type":"markdown","source":"## Data Transformation","metadata":{}},{"cell_type":"markdown","source":"В этом коде используется преобразование временных признаков, таких как час и день в году, в синусоидальные и косинусоидальные значения, чтобы лучше отобразить их циклическую природу. Кроме того, вычисляются статистические характеристики целевой переменной, такие как среднее значение, стандартное отклонение и дисперсия, на основе исторических данных.\n","metadata":{}},{"cell_type":"code","source":"# Преобразование часа дня в синусоидальные и косинусоидальные значения для учета цикличности времени суток\ntrain['sin_hour'] = (np.pi * np.sin(train['hour']) / 12)  # Синусоидальное представление часа дня\ntrain['cos_hour'] = (np.pi * np.cos(train['hour']) / 12)  # Косинусоидальное представление часа дня\n\n# Преобразование дня в году в синусоидальные и косинусоидальные значения для учета цикличности времени в году\ntrain['sin_dayofyear'] = (np.pi * np.sin(train['dayofyear']) / 183)  # Синусоидальное представление дня в году\ntrain['cos_dayofyear'] = (np.pi * np.cos(train['dayofyear']) / 183)  # Косинусоидальное представление дня в году\n\n# Расчет статистических характеристик целевой переменной на основе исторических данных\ntrain['target_mean'] = train[[f'target_{i}_days_ago' for i in range(2, N_day_lags+1)]].mean(1)  # Среднее значение целевой переменной за предыдущие дни\ntrain['target_std'] = train[[f'target_{i}_days_ago' for i in range(2, N_day_lags+1)]].std(1)  # Стандартное отклонение целевой переменной за предыдущие дни\ntrain['target_var'] = train[[f'target_{i}_days_ago' for i in range(2, N_day_lags+1)]].var(1)  # Дисперсия целевой переменной за предыдущие дни\n","metadata":{"execution":{"iopub.status.busy":"2023-12-26T22:12:24.279271Z","iopub.execute_input":"2023-12-26T22:12:24.279961Z","iopub.status.idle":"2023-12-26T22:12:26.270751Z","shell.execute_reply.started":"2023-12-26T22:12:24.279923Z","shell.execute_reply":"2023-12-26T22:12:26.269910Z"},"trusted":true},"execution_count":166,"outputs":[]},{"cell_type":"code","source":"#Log the columns with outliers\nto_log= ['installed_capacity', 'euros_per_mwh', 'temperature_fcast_mean', 'dewpoint_fcast_mean','cloudcover_high_fcast_mean', 'cloudcover_low_fcast_mean', 'cloudcover_mid_fcast_mean', 'cloudcover_total_fcast_mean','10_metre_u_wind_component_fcast_mean', '10_metre_v_wind_component_fcast_mean', 'direct_solar_radiation_fcast_mean','snowfall_fcast_mean', 'total_precipitation_fcast_mean', 'temperature_fcast_mean_by_county', 'dewpoint_fcast_mean_by_county','cloudcover_high_fcast_mean_by_county', 'cloudcover_low_fcast_mean_by_county', 'cloudcover_mid_fcast_mean_by_county','cloudcover_total_fcast_mean_by_county', '10_metre_u_wind_component_fcast_mean_by_county', '10_metre_v_wind_component_fcast_mean_by_county','surface_solar_radiation_downwards_fcast_mean_by_county', 'snowfall_fcast_mean_by_county', 'total_precipitation_fcast_mean_by_county','rain_hist_mean', 'snowfall_hist_mean', 'windspeed_10m_hist_mean_by_county', 'target_2_days_ago', 'target_3_days_ago','target_4_days_ago', 'target_5_days_ago', 'target_6_days_ago', 'target_7_days_ago', 'target_mean', 'target_std']\nfor i in to_log:\n    train[f\"log_{i}\"]= np.where((train[i])!= 0, np.log(train[i]),0)","metadata":{"execution":{"iopub.status.busy":"2023-12-26T22:12:26.272202Z","iopub.execute_input":"2023-12-26T22:12:26.272582Z","iopub.status.idle":"2023-12-26T22:12:26.782570Z","shell.execute_reply.started":"2023-12-26T22:12:26.272544Z","shell.execute_reply":"2023-12-26T22:12:26.781770Z"},"trusted":true},"execution_count":167,"outputs":[]},{"cell_type":"code","source":"train=train[train.year >= 2022]","metadata":{"execution":{"iopub.status.busy":"2023-12-26T22:12:26.783840Z","iopub.execute_input":"2023-12-26T22:12:26.784668Z","iopub.status.idle":"2023-12-26T22:12:27.376865Z","shell.execute_reply.started":"2023-12-26T22:12:26.784630Z","shell.execute_reply":"2023-12-26T22:12:27.376019Z"},"trusted":true},"execution_count":168,"outputs":[]},{"cell_type":"markdown","source":"## Training columns","metadata":{}},{"cell_type":"code","source":"#Storing training features into numpy arrays\nX= train.drop('target', axis= 1).values\ny= train['target']\n\n#Storing production targets into an array itself | Will seperate it into another model\nX2= train[train['is_consumption'] == 0].drop('target', axis= 1).values\ny2= train[train['is_consumption'] == 0]['target']","metadata":{"execution":{"iopub.status.busy":"2023-12-26T22:12:27.378311Z","iopub.execute_input":"2023-12-26T22:12:27.378692Z","iopub.status.idle":"2023-12-26T22:12:29.502501Z","shell.execute_reply.started":"2023-12-26T22:12:27.378655Z","shell.execute_reply":"2023-12-26T22:12:29.501669Z"},"trusted":true},"execution_count":169,"outputs":[]},{"cell_type":"markdown","source":"## Training models","metadata":{}},{"cell_type":"markdown","source":"### MOD1 Params","metadata":{}},{"cell_type":"code","source":"# p1={'n_estimators':4000,'verbose': -1,'random_state':73,'objective':'tweedie','learning_rate': 0.030339736147758608, 'colsample_bytree': 0.9263063801074632, 'colsample_bynode': 0.4527058263857967, 'reg_alpha': 3.62802063709343, 'reg_lambda': 1.6506819544194185, 'min_data_in_leaf': 201, 'max_depth': 15, 'device':'gpu',\"n_jobs\" : 4,'num_leaves': 455}\n# p2={'n_estimators':4000,'verbose': -1,'random_state':73,'objective':'tweedie','learning_rate': 0.033090718804096083, 'colsample_bytree': 0.9499770953943448, 'colsample_bynode': 0.4670163857441046, 'reg_aplha': 3.9694606555680705, 'reg_lambda': 1.925712107567988, 'min_data_in_leaf': 223, 'max_depth': 18, 'device':'gpu',\"n_jobs\" : 4,'num_leaves': 465}\n# p3={'n_estimators':4000,'verbose': -1,'random_state':73,'objective':'tweedie','learning_rate': 0.035559490612977255, 'colsample_bytree': 0.9682791614810814, 'colsample_bynode': 0.4722023075509447, 'reg_aplha': 4.1562458539834125, 'reg_lambda': 2.265053303366992, 'min_data_in_leaf': 254, 'max_depth': 19, 'device':'gpu',\"n_jobs\" : 4,'num_leaves': 475}\n# p4={'n_estimators':4000,'verbose': -1,'random_state':73,'objective':'tweedie','learning_rate': 0.038908744594789185, 'colsample_bytree': 0.9864875442500248, 'colsample_bynode': 0.4832525869590394, 'reg_aplha': 4.358459131925572, 'reg_lambda': 2.355521088983217, 'min_data_in_leaf': 289, 'max_depth': 21, 'device':'gpu',\"n_jobs\" : 4,'num_leaves': 485}","metadata":{"execution":{"iopub.status.busy":"2023-12-26T22:12:29.503874Z","iopub.execute_input":"2023-12-26T22:12:29.504241Z","iopub.status.idle":"2023-12-26T22:12:29.509062Z","shell.execute_reply.started":"2023-12-26T22:12:29.504207Z","shell.execute_reply":"2023-12-26T22:12:29.508072Z"},"trusted":true},"execution_count":170,"outputs":[]},{"cell_type":"code","source":"p1={'n_estimators':4000,'verbose': -1,'random_state':73,'objective':'mae','learning_rate': 0.030339736147758608, 'colsample_bytree': 0.9263063801074632, 'colsample_bynode': 0.4527058263857967, 'reg_alpha': 3.62802063709343, 'reg_lambda': 1.6506819544194185, 'min_data_in_leaf': 201, 'max_depth': 15, 'device':'gpu',\"n_jobs\" : 4,'num_leaves': 455}\np2={'n_estimators':4000,'verbose': -1,'random_state':73,'objective':'mae','learning_rate': 0.033090718804096083, 'colsample_bytree': 0.9499770953943448, 'colsample_bynode': 0.4670163857441046, 'reg_aplha': 3.9694606555680705, 'reg_lambda': 1.925712107567988, 'min_data_in_leaf': 223, 'max_depth': 18, 'device':'gpu',\"n_jobs\" : 4,'num_leaves': 465}\np3={'n_estimators':4000,'verbose': -1,'random_state':73,'objective':'mae','learning_rate': 0.035559490612977255, 'colsample_bytree': 0.9682791614810814, 'colsample_bynode': 0.4722023075509447, 'reg_aplha': 4.1562458539834125, 'reg_lambda': 2.265053303366992, 'min_data_in_leaf': 254, 'max_depth': 19, 'device':'gpu',\"n_jobs\" : 4,'num_leaves': 475}\np4={'n_estimators':4000,'verbose': -1,'random_state':73,'objective':'mae','learning_rate': 0.038908744594789185, 'colsample_bytree': 0.9864875442500248, 'colsample_bynode': 0.4832525869590394, 'reg_aplha': 4.358459131925572, 'reg_lambda': 2.355521088983217, 'min_data_in_leaf': 289, 'max_depth': 21, 'device':'gpu',\"n_jobs\" : 4,'num_leaves': 485}","metadata":{"execution":{"iopub.status.busy":"2023-12-26T22:12:29.513538Z","iopub.execute_input":"2023-12-26T22:12:29.513945Z","iopub.status.idle":"2023-12-26T22:12:29.524914Z","shell.execute_reply.started":"2023-12-26T22:12:29.513920Z","shell.execute_reply":"2023-12-26T22:12:29.523984Z"},"trusted":true},"execution_count":171,"outputs":[]},{"cell_type":"code","source":"# # #LGB\n# p1={'n_estimators': 1000,'verbose': -1,'objective': 'l2','learning_rate': 0.06258413085998576, 'colsample_bytree': 0.6527661140701613, 'colsample_bynode': 0.8106858631408332, 'lambda_l1': 5.065645378814257, 'lambda_l2': 9.81159370218779, 'min_data_in_leaf': 192, 'max_depth': 10, 'max_bin': 1800}\n# p2={'n_estimators': 1000,'verbose': -1,'objective': 'l2','learning_rate': 0.0632167263149817, 'colsample_bytree': 0.6958033941948067, 'colsample_bynode': 0.6030801666196094, 'lambda_l1': 7.137580620471935, 'lambda_l2': 9.348169401713742, 'min_data_in_leaf': 74, 'max_depth': 11, 'max_bin': 530}\n# p3={'n_estimators': 1000,'verbose': -1,'objective': 'l2','learning_rate': 0.061236402165228264, 'colsample_bytree': 0.81427095118471, 'colsample_bynode': 0.6097376843527067, 'lambda_l1': 6.360490880385201, 'lambda_l2': 9.954136008333839, 'min_data_in_leaf': 238, 'max_depth': 13, 'max_bin': 649}\n# p4={'n_estimators': 1000,'verbose': -1,'objective': 'l2','learning_rate': 0.06753282378023663, 'colsample_bytree': 0.7508715107428325, 'colsample_bynode': 0.6831819500325418, 'lambda_l1': 8.679353563755722, 'lambda_l2': 6.105008696961338, 'min_data_in_leaf': 198, 'max_depth': 15, 'max_bin': 835}\n# p5={'n_estimators': 1000,'verbose': -1,'objective': 'l2','learning_rate': 0.05129380742257108, 'colsample_bytree': 0.5101576947777211, 'colsample_bynode': 0.8052639518604396, 'lambda_l1': 8.087311995794915, 'lambda_l2': 5.067361158677095, 'min_data_in_leaf': 222, 'max_depth': 8, 'max_bin': 97}\n# p6={'n_estimators': 900,'verbose': -1,'objective': 'l2','learning_rate': 0.05689066836106983,'colsample_bytree': 0.8915976762048253,'colsample_bynode': 0.5942203285139224,'lambda_l1': 3.6277555139102864,'lambda_l2': 1.6591278779517808,'min_data_in_leaf' : 186,'max_depth': 9,'max_bin': 813,}\n\n# p7={'n_estimators': 1483,'verbose': -1,'objective': 'l2','learning_rate': 0.047463300970785334,'colsample_bytree': 0.5765687465069222,'colsample_bynode': 0.745770069784652,'lambda_l1': 5.569745853175777,'lambda_l2': 0.9051759724463506,'min_data_in_leaf' : 186,'max_depth': 11,'max_bin': 738,}\n# # p7={'min_data_in_leaf': 138,   'num_leaves': 68, 'feature_fraction': 0.8935903906747225, 'bagging_fraction': 0.9688930607563444, 'bagging_freq': 1}","metadata":{"execution":{"iopub.status.busy":"2023-12-26T22:12:29.525994Z","iopub.execute_input":"2023-12-26T22:12:29.526231Z","iopub.status.idle":"2023-12-26T22:12:29.537692Z","shell.execute_reply.started":"2023-12-26T22:12:29.526209Z","shell.execute_reply":"2023-12-26T22:12:29.536938Z"},"trusted":true},"execution_count":172,"outputs":[]},{"cell_type":"code","source":"lgbp1=LGBMRegressor(**p1)\nlgbp2=LGBMRegressor(**p2)\nlgbp3=LGBMRegressor(**p3)\nlgbp4=LGBMRegressor(**p4)","metadata":{"execution":{"iopub.status.busy":"2023-12-26T22:12:29.538612Z","iopub.execute_input":"2023-12-26T22:12:29.538979Z","iopub.status.idle":"2023-12-26T22:12:29.551519Z","shell.execute_reply.started":"2023-12-26T22:12:29.538955Z","shell.execute_reply":"2023-12-26T22:12:29.550606Z"},"trusted":true},"execution_count":173,"outputs":[]},{"cell_type":"markdown","source":"### MOD2 params","metadata":{}},{"cell_type":"code","source":"# n1={'n_iter':4000,'verbose': -1,'random_state':73,'objective':'tweedie','learning_rate': 0.010339736147758608, 'colsample_bytree': 0.8893063801074632, 'colsample_bynode': 0.4527058263857967, 'lambda_l1': 3.62802063709343, 'lambda_l2': 1.6506819544194185, 'min_data_in_leaf': 63, 'max_depth': 12, 'device':'gpu', 'min_data_per_groups': 59,'num_leaves': 455,\"n_jobs\" : 4}\n# n2={'n_iter':4000,'verbose': -1,'random_state':73,'objective':'tweedie','learning_rate': 0.012090718804096083, 'colsample_bytree': 0.9099770953943448, 'colsample_bynode': 0.4670163857441046, 'lambda_l1': 3.8694606555680705, 'lambda_l2': 1.925712107567988, 'min_data_in_leaf': 68, 'max_depth': 14, 'device':'gpu', 'min_data_per_groups': 69,'num_leaves': 465,\"n_jobs\" : 4}\n# n3={'n_iter':4000,'verbose': -1,'random_state':73,'objective':'tweedie','learning_rate': 0.014559490612977255, 'colsample_bytree': 0.9282791614810814, 'colsample_bynode': 0.4722023075509447, 'lambda_l1': 4.0562458539834125, 'lambda_l2': 2.265053303366992, 'min_data_in_leaf': 73, 'max_depth': 17, 'device':'gpu', 'min_data_per_groups': 79,'num_leaves': 475,\"n_jobs\" : 4}\n# n4={'n_iter':4000,'verbose': -1,'random_state':73,'objective':'tweedie','learning_rate': 0.016908744594789185, 'colsample_bytree': 0.9534875442500248, 'colsample_bynode': 0.4832525869590394, 'lambda_l1': 4.258459131925572, 'lambda_l2': 2.355521088983217, 'min_data_in_leaf': 78, 'max_depth': 21, 'device':'gpu', 'min_data_per_groups': 89,'num_leaves': 485,\"n_jobs\" : 4}","metadata":{"execution":{"iopub.status.busy":"2023-12-26T22:12:29.553431Z","iopub.execute_input":"2023-12-26T22:12:29.553745Z","iopub.status.idle":"2023-12-26T22:12:29.562223Z","shell.execute_reply.started":"2023-12-26T22:12:29.553696Z","shell.execute_reply":"2023-12-26T22:12:29.561479Z"},"trusted":true},"execution_count":174,"outputs":[]},{"cell_type":"code","source":"# !pip install catboost[gpu]","metadata":{"execution":{"iopub.status.busy":"2023-12-26T22:12:29.563164Z","iopub.execute_input":"2023-12-26T22:12:29.563417Z","iopub.status.idle":"2023-12-26T22:12:29.575421Z","shell.execute_reply.started":"2023-12-26T22:12:29.563394Z","shell.execute_reply":"2023-12-26T22:12:29.574584Z"},"trusted":true},"execution_count":175,"outputs":[]},{"cell_type":"code","source":"# !pip install catboost-cuda\n","metadata":{"execution":{"iopub.status.busy":"2023-12-26T22:12:29.576600Z","iopub.execute_input":"2023-12-26T22:12:29.577307Z","iopub.status.idle":"2023-12-26T22:12:29.584742Z","shell.execute_reply.started":"2023-12-26T22:12:29.577265Z","shell.execute_reply":"2023-12-26T22:12:29.583952Z"},"trusted":true},"execution_count":176,"outputs":[]},{"cell_type":"code","source":"import catboost as cb  # Библиотека для градиентного бустинга","metadata":{"execution":{"iopub.status.busy":"2023-12-26T22:12:29.585704Z","iopub.execute_input":"2023-12-26T22:12:29.586007Z","iopub.status.idle":"2023-12-26T22:12:29.600593Z","shell.execute_reply.started":"2023-12-26T22:12:29.585984Z","shell.execute_reply":"2023-12-26T22:12:29.599763Z"},"trusted":true},"execution_count":177,"outputs":[]},{"cell_type":"code","source":"c1={'iterations': 1800,'eval_metric': 'MAE','learning_rate': 0.04094387403153919, 'depth': 9, 'l2_leaf_reg': 3, 'border_count': 160, 'random_strength': 2, 'bagging_temperature': 0.30238224812191056}\nc2={'iterations': 1800,'eval_metric': 'MAE','learning_rate': 0.06054396656784583, 'depth': 11, 'l2_leaf_reg': 6, 'border_count': 134, 'random_strength': 9, 'bagging_temperature': 0.29607411641626996}\nc3 = {\n    'learning_rate': 0.12358952478027072,\n    'depth': 11,\n    'l2_leaf_reg': 8,\n    'border_count': 191,\n    'random_strength': 3,\n    'bagging_temperature': 0.41774414265586035,\n    'iterations': 1800,\n    'eval_metric': 'MAE'\n}\n\nc4 = {\n    'learning_rate': 0.06258413085998576,\n    'depth': 10,\n    'l2_leaf_reg': 8,\n    'border_count': 211,\n    'random_strength': 6,\n    'bagging_temperature': 0.13029094645654574,\n    'iterations': 1800,\n    'eval_metric': 'MAE'\n}","metadata":{"execution":{"iopub.status.busy":"2023-12-26T22:12:29.601765Z","iopub.execute_input":"2023-12-26T22:12:29.602081Z","iopub.status.idle":"2023-12-26T22:12:29.611583Z","shell.execute_reply.started":"2023-12-26T22:12:29.602052Z","shell.execute_reply":"2023-12-26T22:12:29.610784Z"},"trusted":true},"execution_count":178,"outputs":[]},{"cell_type":"code","source":"# c1={'task_type': 'gpu','learning_rate': 0.04094387403153919, 'depth': 9, 'l2_leaf_reg': 3, 'border_count': 160, 'random_strength': 2, 'bagging_temperature': 0.30238224812191056}\n# c2={'task_type': 'gpu','learning_rate': 0.06054396656784583, 'depth': 11, 'l2_leaf_reg': 6, 'border_count': 134, 'random_strength': 9, 'bagging_temperature': 0.29607411641626996}\n# c3 = {\n#     'learning_rate': 0.12358952478027072,\n#     'depth': 11,\n#     'l2_leaf_reg': 8,\n#     'border_count': 191,\n#     'random_strength': 3,\n#     'bagging_temperature': 0.41774414265586035,\n#     'iterations': 1800,\n#     'eval_metric': 'MAE',\n#     'task_type': 'gpu',# Добавление поддержки GPU\n#     'cat_features': ['county', 'is_business', 'product_type', 'is_consumption', 'category_1']\n# }\n\n# c4 = {\n#     'learning_rate': 0.06258413085998576,\n#     'depth': 10,\n#     'l2_leaf_reg': 8,\n#     'border_count': 211,\n#     'random_strength': 6,\n#     'bagging_temperature': 0.13029094645654574,\n#     'iterations': 1800,\n#     'eval_metric': 'MAE',\n#     'task_type': 'gpu',\n#     'cat_features': ['county', 'is_business', 'product_type', 'is_consumption', 'category_1']\n# }","metadata":{"execution":{"iopub.status.busy":"2023-12-26T22:12:29.612923Z","iopub.execute_input":"2023-12-26T22:12:29.613244Z","iopub.status.idle":"2023-12-26T22:12:29.626216Z","shell.execute_reply.started":"2023-12-26T22:12:29.613211Z","shell.execute_reply":"2023-12-26T22:12:29.625261Z"},"trusted":true},"execution_count":179,"outputs":[]},{"cell_type":"code","source":"cat1=cb.CatBoostRegressor(**c1, random_state=42)\ncat2=cb.CatBoostRegressor(**c2, random_state=42)\ncat3=cb.CatBoostRegressor(**c3, random_state=42)\ncat4=cb.CatBoostRegressor(**c4, random_state=42)\n\n        ","metadata":{"execution":{"iopub.status.busy":"2023-12-26T22:12:29.628035Z","iopub.execute_input":"2023-12-26T22:12:29.628310Z","iopub.status.idle":"2023-12-26T22:12:29.639286Z","shell.execute_reply.started":"2023-12-26T22:12:29.628287Z","shell.execute_reply":"2023-12-26T22:12:29.638378Z"},"trusted":true},"execution_count":180,"outputs":[]},{"cell_type":"code","source":"# lgbn1=LGBMRegressor(**n1)\n# lgbn2=LGBMRegressor(**n2)\n# lgbn3=LGBMRegressor(**n3)\n# lgbn4=LGBMRegressor(**n4)","metadata":{"execution":{"iopub.status.busy":"2023-12-26T22:12:29.640583Z","iopub.execute_input":"2023-12-26T22:12:29.640941Z","iopub.status.idle":"2023-12-26T22:12:29.648870Z","shell.execute_reply.started":"2023-12-26T22:12:29.640909Z","shell.execute_reply":"2023-12-26T22:12:29.647913Z"},"trusted":true},"execution_count":181,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n# Split your data\nXtr, Xval, ytr, yval = train_test_split(X, y, test_size=0.2, random_state=73,shuffle=True)\nX2tr, X2val, y2tr, y2val = train_test_split(X2, y2, test_size=0.2, random_state=73,shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-26T22:12:29.649904Z","iopub.execute_input":"2023-12-26T22:12:29.650674Z","iopub.status.idle":"2023-12-26T22:12:33.213127Z","shell.execute_reply.started":"2023-12-26T22:12:29.650648Z","shell.execute_reply":"2023-12-26T22:12:33.212119Z"},"trusted":true},"execution_count":182,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"for lgbm_model in [lgbp1, lgbp2, lgbp3, lgbp4]:\n    print('_______________________________________________________')\n    print('Start')\n    lgbm_model.fit(Xtr, ytr, eval_set=[(Xval, yval)], callbacks=[\n            lgb.callback.early_stopping(stopping_rounds=100),\n            lgb.callback.log_evaluation(period=100),\n        ],)","metadata":{"execution":{"iopub.status.busy":"2023-12-26T22:12:33.214458Z","iopub.execute_input":"2023-12-26T22:12:33.214855Z","iopub.status.idle":"2023-12-26T22:13:23.863227Z","shell.execute_reply.started":"2023-12-26T22:12:33.214819Z","shell.execute_reply":"2023-12-26T22:13:23.861761Z"},"trusted":true},"execution_count":183,"outputs":[{"name":"stdout","text":"_______________________________________________________\nStart\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's l1: 65.1581\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[183], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_______________________________________________________\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStart\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mlgbm_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXtr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mytr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myval\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mperiod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:895\u001b[0m, in \u001b[0;36mLGBMRegressor.fit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y,\n\u001b[1;32m    889\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, init_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    890\u001b[0m         eval_set\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, eval_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, eval_sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    891\u001b[0m         eval_init_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    892\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m'\u001b[39m, feature_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, categorical_feature\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    893\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, init_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    894\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 895\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m                \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m                \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m                \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    900\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:748\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    745\u001b[0m evals_result \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    746\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[0;32m--> 748\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evals_result:\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evals_result \u001b[38;5;241m=\u001b[39m evals_result\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py:292\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[1;32m    285\u001b[0m     cb(callback\u001b[38;5;241m.\u001b[39mCallbackEnv(model\u001b[38;5;241m=\u001b[39mbooster,\n\u001b[1;32m    286\u001b[0m                             params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    287\u001b[0m                             iteration\u001b[38;5;241m=\u001b[39mi,\n\u001b[1;32m    288\u001b[0m                             begin_iteration\u001b[38;5;241m=\u001b[39minit_iteration,\n\u001b[1;32m    289\u001b[0m                             end_iteration\u001b[38;5;241m=\u001b[39minit_iteration \u001b[38;5;241m+\u001b[39m num_boost_round,\n\u001b[1;32m    290\u001b[0m                             evaluation_result_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m--> 292\u001b[0m \u001b[43mbooster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m evaluation_result_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py:3021\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   3019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_objective_to_none:\n\u001b[1;32m   3020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot update due to null objective function.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 3021\u001b[0m _safe_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_BoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3022\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_finished\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   3024\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__is_predicted_cur_iter \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)]\n\u001b[1;32m   3025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-12-26T22:13:23.864515Z","iopub.status.idle":"2023-12-26T22:13:23.864933Z","shell.execute_reply.started":"2023-12-26T22:13:23.864743Z","shell.execute_reply":"2023-12-26T22:13:23.864766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for lgbm_model in [lgbn1, lgbn2, lgbn3, lgbn4]:\n#     print('_______________________________________________________')\n#     print('Start')\n#     lgbm_model.fit(X2tr, y2tr, eval_set=[(X2val, y2val)], callbacks=[\n#             lgb.callback.early_stopping(stopping_rounds=100),\n#             lgb.callback.log_evaluation(period=100),\n#         ],)","metadata":{"execution":{"iopub.status.busy":"2023-12-26T22:13:23.866745Z","iopub.status.idle":"2023-12-26T22:13:23.867224Z","shell.execute_reply.started":"2023-12-26T22:13:23.866981Z","shell.execute_reply":"2023-12-26T22:13:23.867003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfor cat_model in [cat1, cat2, cat3, cat4]:\n    print('_______________________________________________________')\n    print('Start')\n\n    # Обучение модели CatBoost\n    cat_model.fit(\n        X2tr, y2tr, \n        eval_set=[(X2val, y2val)], \n        early_stopping_rounds=100, \n        verbose=100  # Вывод информации каждые 100 итераций\n    ) ","metadata":{"execution":{"iopub.status.busy":"2023-12-26T22:13:23.869108Z","iopub.status.idle":"2023-12-26T22:13:23.869470Z","shell.execute_reply.started":"2023-12-26T22:13:23.869298Z","shell.execute_reply":"2023-12-26T22:13:23.869315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-12-26T22:13:23.870583Z","iopub.status.idle":"2023-12-26T22:13:23.870958Z","shell.execute_reply.started":"2023-12-26T22:13:23.870777Z","shell.execute_reply":"2023-12-26T22:13:23.870801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering for Test Data","metadata":{"execution":{"iopub.status.busy":"2023-11-19T18:26:07.884215Z","iopub.execute_input":"2023-11-19T18:26:07.884617Z","iopub.status.idle":"2023-11-19T18:26:07.889926Z","shell.execute_reply.started":"2023-11-19T18:26:07.884584Z","shell.execute_reply":"2023-11-19T18:26:07.888991Z"}}},{"cell_type":"code","source":"def feat_eng_test(data, client, hist_weather, forecast_weather, electricity, gas, locations):\n    \n    data= data.rename(columns={'prediction_datetime' : 'datetime'})\n    data['datetime'] = pd.to_datetime(data['datetime'], utc=True)\n        \n    electricity = electricity.rename(columns= {'forecast_date' : 'datetime'})    \n    electricity['datetime'] = pd.to_datetime(electricity['datetime'], utc= True)\n\n    locations = locations.drop('Unnamed: 0', axis= 1) \n        \n    forecast_weather[['latitude', 'longitude']] = forecast_weather[['latitude', 'longitude']].astype(float).round(1)   \n    forecast_weather= forecast_weather.merge(locations, how='left', on=['longitude','latitude'])\n    forecast_weather.dropna(axis= 0, inplace= True)    \n    forecast_weather['county'] = forecast_weather['county'].astype('int64')\n    forecast_weather.drop(['origin_datetime', 'latitude', 'longitude', 'hours_ahead', 'data_block_id'], axis=1, inplace= True)\n    forecast_weather.rename(columns={'forecast_datetime': 'datetime'}, inplace= True)\n    forecast_weather['datetime']= pd.to_datetime(forecast_weather['datetime'], utc= True)\n    \n    forecast_weather_datetime= forecast_weather.groupby([forecast_weather['datetime'].dt.to_period('h')])[list(forecast_weather.drop(['county','datetime'], axis= 1).columns)].mean().reset_index()\n    forecast_weather_datetime['datetime']= pd.to_datetime(forecast_weather_datetime['datetime'].dt.to_timestamp(), utc=True)\n    forecast_weather_datetime_county= forecast_weather.groupby(['county',forecast_weather['datetime'].dt.to_period('h')])[list(forecast_weather.drop(['county','datetime'], axis= 1).columns)].mean().reset_index()\n    forecast_weather_datetime_county['datetime']= pd.to_datetime(forecast_weather_datetime_county['datetime'].dt.to_timestamp(), utc=True)\n          \n    hist_weather[['latitude', 'longitude']] = hist_weather[['latitude', 'longitude']].astype(float).round(1)\n    hist_weather= hist_weather.merge(locations, how='left', on=['longitude','latitude'])    \n    hist_weather.dropna(axis= 0, inplace= True)\n    hist_weather.drop(['latitude', 'longitude'], axis=1, inplace= True)\n    hist_weather['county'] = hist_weather['county'].astype('int64')\n    hist_weather['datetime']= pd.to_datetime(hist_weather['datetime'], utc= True)\n    hist_weather_datetime= hist_weather.groupby([hist_weather['datetime'].dt.to_period('h')])[list(hist_weather.drop(['county','datetime', 'data_block_id'], axis= 1).columns)].mean().reset_index()    \n    hist_weather_datetime['datetime']= pd.to_datetime(hist_weather_datetime['datetime'].dt.to_timestamp(), utc=True)\n    hist_weather_datetime= hist_weather_datetime.merge(hist_weather[['datetime', 'data_block_id']], how='left', on='datetime')\n    hist_weather_datetime_county= hist_weather.groupby(['county',hist_weather['datetime'].dt.to_period('h')])[list(hist_weather.drop(['county','datetime', 'data_block_id'], axis= 1).columns)].mean().reset_index() \n    hist_weather_datetime_county['datetime']= pd.to_datetime(hist_weather_datetime_county['datetime'].dt.to_timestamp(), utc=True)\n    hist_weather_datetime_county= hist_weather_datetime_county.merge(hist_weather[['datetime', 'data_block_id']], how='left', on='datetime')\n    \n    data['year'] = data['datetime'].dt.year \n    data['month'] = data['datetime'].dt.month\n    data['day'] = data['datetime'].dt.day\n    data['hour'] = data['datetime'].dt.hour\n    data['dayofweek']= data['datetime'].dt.dayofweek\n    data['dayofyear']= data['datetime'].dt.dayofyear\n        \n    electricity['hour'] = electricity['datetime'].dt.hour\n\n    data= data.merge(client.drop(columns = ['date']), how='left', on=['data_block_id', 'county', 'is_business', 'product_type'])\n    data= data.merge(gas[['data_block_id', 'lowest_price_per_mwh', 'highest_price_per_mwh']], how='left', on='data_block_id')\n    data= data.merge(electricity[['euros_per_mwh', 'hour', 'data_block_id']], how='left', on=['hour', 'data_block_id'])\n    data= data.merge(forecast_weather_datetime, how='left', on=['datetime'])\n    data= data.merge(forecast_weather_datetime_county, how='left', on=['datetime', 'county'],suffixes= ('_fcast_mean','_fcast_mean_by_county')) \n    \n    hist_weather_datetime['hour']= hist_weather_datetime['datetime'].dt.hour\n    hist_weather_datetime_county['hour']= hist_weather_datetime_county['datetime'].dt.hour\n    hist_weather_datetime.drop_duplicates(inplace=True)\n    hist_weather_datetime_county.drop_duplicates(inplace=True)\n    hist_weather_datetime.drop('datetime', axis= 1, inplace= True)\n    hist_weather_datetime_county.drop('datetime', axis= 1, inplace= True)\n\n    data= data.merge(hist_weather_datetime, how='left', on=['data_block_id', 'hour'])\n    data= data.merge(hist_weather_datetime_county, how='left', on=['data_block_id', 'county', 'hour'],suffixes= ('_hist_mean','_hist_mean_by_county'))\n    data= data.groupby(['year', 'day', 'hour'], as_index=False).apply(lambda x: x.ffill().bfill()).reset_index()\n    data.drop(['level_0', 'level_1', 'row_id', 'data_block_id'], axis= 1, inplace= True)\n    \n    return data","metadata":{"execution":{"iopub.status.busy":"2023-12-26T22:13:23.873104Z","iopub.status.idle":"2023-12-26T22:13:23.873671Z","shell.execute_reply.started":"2023-12-26T22:13:23.873421Z","shell.execute_reply":"2023-12-26T22:13:23.873444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_revealed_targets_test(data, previous_revealed_targets, N_day_lags):\n    # Create new test data based on previous_revealed_targets and N_day_lags \n    \n    for count, revealed_targets in enumerate(previous_revealed_targets) :\n        day_lag = count + 2\n        revealed_targets['hour'] = pd.to_datetime(revealed_targets['datetime'], utc= True).dt.hour\n        \n        revealed_targets = revealed_targets[['hour', 'prediction_unit_id', 'is_consumption', 'target']]\n        revealed_targets = revealed_targets.rename(columns = {\"target\" : f\"target_{day_lag}_days_ago\"})\n        data = pd.merge(data,revealed_targets,how = 'left',on = ['hour', 'prediction_unit_id', 'is_consumption'],)\n        \n    all_revealed_columns = [f\"target_{day_lag}_days_ago\" for day_lag in range(2, N_day_lags+1)]\n    missing_columns = list(set(all_revealed_columns) - set(data.columns))\n    data[missing_columns] = np.nan \n    \n    return data","metadata":{"execution":{"iopub.status.busy":"2023-12-26T22:13:23.874920Z","iopub.status.idle":"2023-12-26T22:13:23.875384Z","shell.execute_reply.started":"2023-12-26T22:13:23.875138Z","shell.execute_reply":"2023-12-26T22:13:23.875160Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{"execution":{"iopub.status.busy":"2023-12-17T19:52:39.007305Z","iopub.execute_input":"2023-12-17T19:52:39.007738Z","iopub.status.idle":"2023-12-17T19:52:39.012084Z","shell.execute_reply.started":"2023-12-17T19:52:39.007703Z","shell.execute_reply":"2023-12-17T19:52:39.011305Z"}}},{"cell_type":"code","source":"previous_revealed_targets = []\nenv = enefit.make_env()\niter_test = env.iter_test()\nfor (test, revealed_targets, client_test, historical_weather_test,\n     forecast_weather_test, electricity_test, gas_test,sample_prediction) in iter_test:\n    \n    # Rename test set to make consistent with train\n    test = test.rename(columns = {'prediction_datetime': 'datetime'})\n    \n    id_column = 'data_block_id' \n    \n    test[id_column] = 0\n    gas_test[id_column] = 0\n    electricity_test[id_column] = 0\n    historical_weather_test[id_column] = 0\n    forecast_weather_test[id_column] = 0\n    client_test[id_column] = 0\n    revealed_targets[id_column] = 0\n    \n    data_test = feat_eng_test(test, client_test, historical_weather_test,forecast_weather_test, electricity_test, gas_test, locations)\n    \n    data_test['datetime']= pd.to_datetime(data_test['datetime'], utc= True).astype('int64')\n    \n    previous_revealed_targets.insert(0, revealed_targets)\n    if len(previous_revealed_targets) == N_day_lags:\n        previous_revealed_targets.pop()\n    \n    df_test = create_revealed_targets_test(data = data_test.copy(),previous_revealed_targets = previous_revealed_targets.copy(), N_day_lags = N_day_lags)\n    \n    #Data Transformation\n    df_test['sin_hour']= (np.pi * np.sin(df_test['hour']) / 12)\n    df_test['cos_hour']= (np.pi * np.cos(df_test['hour']) / 12)\n    df_test['sin_hour']= (np.pi * np.sin(df_test['hour']) / 12)\n    df_test['cos_hour']= (np.pi * np.cos(df_test['hour']) / 12)\n    df_test['sin_dayofyear']= (np.pi * np.sin(df_test['dayofyear']) / 183)\n    df_test['cos_dayofyear']= (np.pi * np.cos(df_test['dayofyear']) / 183)\n    df_test['target_mean']= df_test[[f'target_{i}_days_ago' for i in range(2, N_day_lags+1)]].mean(1)\n    df_test['target_std']= df_test[[f'target_{i}_days_ago' for i in range(2, N_day_lags+1)]].std(1)\n    df_test['target_var']= df_test[[f'target_{i}_days_ago' for i in range(2, N_day_lags+1)]].var(1)\n    for i in to_log:\n        df_test[f\"log_{i}\"]= np.where((df_test[i])!= 0, np.log(df_test[i]),0)\n    X_test = df_test.drop('currently_scored', axis= 1).values\n    \n    # Предсказания\n    # lgbp///////////////////////////\n    # Создание списка для хранения предсказаний каждой модели\n    target_list = []\n    # Проход по списку моделей и добавление предсказаний каждой модели в список\n    for mod in [lgbp1, lgbp2, lgbp3, lgbp4]:\n        # Добавление предсказаний модели в список target_list\n        # Вызов метода .predict() для модели и применение метода .clip(0) для ограничения предсказаний минимальным значением 0\n        target_list.append(mod.predict(X_test).clip(0))\n\n    \n        # Вычисление взвешенного среднего для предсказаний\n    # Каждое предсказание из target_list умножается на свой вес и суммируется для получения итогового предсказания\n    pred = (target_list[0] * 0.3) + (target_list[1] * 0.27) + (target_list[2] * 0.23) + (target_list[3] * 0.2)\n    # Присваивание итогового предсказания столбцу 'target' в DataFrame test\n    test['target'] = pred\n    # cat///////////////////////////\n    # Повторение процесса выше для целевой переменной, связанной с солнечной энергией\n    tsolar_list = []\n    # Применение каждой модели CatBoost для прогнозирования и добавление результатов в список tsolar_list\n    for model in [cat1, cat2, cat3, cat4]:\n        tsolar_list.append(model.predict(X_test).clip(0))  # Прогноз и ограничение его нижней границей на 0\n\n    # Вычисление взвешенного среднего для предсказаний\n    pred_solar = (tsolar_list[0] * 0.25) + (tsolar_list[1] * 0.25) + (tsolar_list[2] * 0.25) + (tsolar_list[3] * 0.25)\n\n    # Присваивание итогового предсказания столбцу 'target_solar' в DataFrame test\n    test['target_solar'] = pred_solar\n\n    \n#     # Повторение процесса выше для целевой переменной, связанной с солнечной энергией\n#     tsolar_list = []\n#     # Применение каждой модели для прогнозирования и добавление результатов в список tsolar_list\n#     for mod in [lgbn1, lgbn2, lgbn3, lgbn4]:\n#         tsolar_list.append(mod.predict(X_test).clip(0))  # Прогноз и ограничение его нижней границей на 0\n#     # Вычисление взвешенного среднего для предсказаний солнечной энергии\n#     pred_solar = (tsolar_list[0] * 0.3) + (tsolar_list[1] * 0.27) + (tsolar_list[2] * 0.23) + (tsolar_list[3] * 0.2)\n#     # Присваивание итогового предсказания солнечной энергии столбцу 'target_solar' в DataFrame test\n#     test['target_solar'] = pred_solar\n\n    \n    gc.collect()\n    \n    test.loc[test['is_consumption']==0, \"target\"] = test.loc[test['is_consumption']==0, \"target_solar\"]  \n    sample_prediction[\"target\"] = test['target']\n    \n    #Sending predictions to the API\n    env.predict(sample_prediction)","metadata":{"execution":{"iopub.status.busy":"2023-12-26T22:13:23.877061Z","iopub.status.idle":"2023-12-26T22:13:23.877519Z","shell.execute_reply.started":"2023-12-26T22:13:23.877285Z","shell.execute_reply":"2023-12-26T22:13:23.877307Z"},"trusted":true},"execution_count":null,"outputs":[]}]}