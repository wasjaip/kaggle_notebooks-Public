{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":56167,"databundleVersionId":6535361,"sourceType":"competition"}],"dockerImageVersionId":30554,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport subprocess\n\nimport pandas as pd\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision.models import resnet18\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn.functional as F\nimport torch.nn.utils.prune as prune\nfrom math import sqrt\nimport json\nfrom copy import deepcopy\n\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'  ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-01T01:01:22.531239Z","iopub.execute_input":"2023-12-01T01:01:22.531544Z","iopub.status.idle":"2023-12-01T01:01:26.734786Z","shell.execute_reply.started":"2023-12-01T01:01:22.531514Z","shell.execute_reply":"2023-12-01T01:01:26.733686Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"https://www.kaggle.com/sebastianoleszko","metadata":{}},{"cell_type":"markdown","source":"<img src='https://unlearning-challenge.github.io/Unlearning-logo.png' width='100px'>\n\n# NeurIPS 2023 Machine Unlearning Challenge Starting Kit\n\n[![Open in Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/unlearning-challenge/starting-kit/main/unlearning-CIFAR10.ipynb)\n\n\nThis notebook is part of the starting kit for the [NeurIPS 2023 Machine Unlearning Challenge](https://unlearning-challenge.github.io/). This notebook explains the pipeline of the challenge and contains sample unlearning and evaluation code.\n\n\nThis notebook has 3 sections:\n\n  * üíæ In the first section we'll load a sample dataset (CIFAR10) and pre-trained model (ResNet18).\n\n  * üéØ In the second section we'll develop the unlearning algorithm. We start by splitting the original training set into a retain set and a forget set. The goal of an unlearning algorithm is to update the pre-trained model so that it approximates as much as possible a model that has been trained on the retain set but not on the forget set. We provide a simple unlearning algorithm as a starting point for participants to develop their own unlearning algorithms.\n\n  * üèÖ In the third section we'll score our unlearning algorithm using a simple membership inference attacks (MIA). Note that this is a different evaluation than the one that will be used in the competition's submission.\n  \n\nWe emphasize that this notebook is provided for convenience to help participants quickly get started. Submissions will be scored using a different method than the one provided in this notebook on a different (private) dataset of human faces. To run the notebook, the requirement is to have installed an up-to-date version of Python and Pytorch.","metadata":{}},{"cell_type":"code","source":"# It's really important to add an accelerator to your notebook, as otherwise the submission will fail.\n# We recomment using the P100 GPU rather than T4 as it's faster and will increase the chances of passing the time cut-off threshold.\n\nif DEVICE != 'cuda':\n    raise RuntimeError('Make sure you have added an accelerator to your notebook; the submission will fail otherwise!')","metadata":{"execution":{"iopub.status.busy":"2023-12-01T01:01:26.737309Z","iopub.execute_input":"2023-12-01T01:01:26.738316Z","iopub.status.idle":"2023-12-01T01:01:26.743021Z","shell.execute_reply.started":"2023-12-01T01:01:26.738279Z","shell.execute_reply":"2023-12-01T01:01:26.742115Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Helper functions for loading the hidden dataset.\n\ndef load_example(df_row):\n    image = torchvision.io.read_image(df_row['image_path'])\n    result = {\n        'image': image,\n        'image_id': df_row['image_id'],\n        'age_group': df_row['age_group'],\n        'age': df_row['age'],\n        'person_id': df_row['person_id']\n    }\n    return result\n\n\nclass HiddenDataset(Dataset):\n    '''The hidden dataset.'''\n    def __init__(self, split='train'):\n        super().__init__()\n        self.examples = []\n\n        df = pd.read_csv(f'/kaggle/input/neurips-2023-machine-unlearning/{split}.csv')\n        df['image_path'] = df['image_id'].apply(\n            lambda x: os.path.join('/kaggle/input/neurips-2023-machine-unlearning/', 'images', x.split('-')[0], x.split('-')[1] + '.png'))\n        df = df.sort_values(by='image_path')\n        df.apply(lambda row: self.examples.append(load_example(row)), axis=1)\n        if len(self.examples) == 0:\n            raise ValueError('No examples.')\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, idx):\n        example = self.examples[idx]\n        image = example['image']\n        image = image.to(torch.float32)\n        example['image'] = image\n        return example\n\n\ndef get_dataset(batch_size):\n    '''Get the dataset.'''\n    retain_ds = HiddenDataset(split='retain')\n    forget_ds = HiddenDataset(split='forget')\n    val_ds = HiddenDataset(split='validation')\n\n    retain_loader = DataLoader(retain_ds, batch_size=batch_size, shuffle=True)\n    forget_loader = DataLoader(forget_ds, batch_size=batch_size, shuffle=True)\n    validation_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=True)\n\n    return retain_loader, forget_loader, validation_loader","metadata":{"execution":{"iopub.status.busy":"2023-12-01T01:01:26.744698Z","iopub.execute_input":"2023-12-01T01:01:26.745251Z","iopub.status.idle":"2023-12-01T01:01:26.763731Z","shell.execute_reply.started":"2023-12-01T01:01:26.745217Z","shell.execute_reply":"2023-12-01T01:01:26.762556Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def kl_loss_fn(outputs, dist_target):\n    kl_loss = F.kl_div(torch.log_softmax(outputs, dim=1), dist_target, log_target=True, reduction='batchmean')\n    return kl_loss","metadata":{"execution":{"iopub.status.busy":"2023-12-01T01:01:26.766491Z","iopub.execute_input":"2023-12-01T01:01:26.766870Z","iopub.status.idle":"2023-12-01T01:01:26.777850Z","shell.execute_reply.started":"2023-12-01T01:01:26.766834Z","shell.execute_reply":"2023-12-01T01:01:26.776910Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def entropy_loss_fn(outputs, labels, dist_target, class_weights):\n    ce_loss = F.cross_entropy(outputs, labels, weight=class_weights)\n    entropy_dist_target = torch.sum(-torch.exp(dist_target) * dist_target, dim=1)\n    entropy_outputs = torch.sum(-torch.softmax(outputs, dim=1) * torch.log_softmax(outputs, dim=1), dim=1)\n    entropy_loss = F.mse_loss(entropy_outputs, entropy_dist_target)\n    return ce_loss + entropy_loss ","metadata":{"execution":{"iopub.status.busy":"2023-12-01T01:01:26.779131Z","iopub.execute_input":"2023-12-01T01:01:26.779807Z","iopub.status.idle":"2023-12-01T01:01:26.788509Z","shell.execute_reply.started":"2023-12-01T01:01:26.779775Z","shell.execute_reply":"2023-12-01T01:01:26.787536Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# You can replace the below simple unlearning with your own unlearning function.\n\ndef unlearning(\n    net, \n    retain_loader, \n    forget_loader, \n    val_loader,\n    class_weights=None,\n):\n    \"\"\"Simple unlearning by finetuning.\"\"\"\n    epochs = 3.2\n    max_iters = int(len(retain_loader) * epochs)\n    optimizer = optim.SGD(net.parameters(), lr=0.0005,\n                      momentum=0.9, weight_decay=5e-4)\n    initial_net = deepcopy(net)\n    \n    net.train()\n    initial_net.eval()\n    \n    def prune_model(net, amount=0.95, rand_init=True):\n        # Modules to prune\n        modules = list()\n        for k, m in enumerate(net.modules()):\n            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n                modules.append((m, 'weight'))\n                if m.bias is not None:\n                    modules.append((m, 'bias'))\n\n        # Prune criteria\n        prune.global_unstructured(\n            modules,\n            #pruning_method=prune.RandomUnstructured,\n            pruning_method=prune.L1Unstructured,\n            amount=amount,\n        )\n\n        # Perform the prune\n        for k, m in enumerate(net.modules()):\n            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n                prune.remove(m, 'weight')\n                if m.bias is not None:\n                    prune.remove(m, 'bias')\n\n        # Random initialization\n        if rand_init:\n            for k, m in enumerate(net.modules()):\n                if isinstance(m, nn.Conv2d):\n                    mask = m.weight == 0\n                    c_in = mask.shape[1]\n                    k = 1/(c_in*mask.shape[2]*mask.shape[3])\n                    randinit = (torch.rand_like(m.weight)-0.5)*2*sqrt(k)\n                    m.weight.data[mask] = randinit[mask]\n                if isinstance(m, nn.Linear):\n                    mask = m.weight == 0\n                    c_in = mask.shape[1]\n                    k = 1/c_in\n                    randinit = (torch.rand_like(m.weight)-0.5)*2*sqrt(k)\n                    m.weight.data[mask] = randinit[mask]\n    \n    num_iters = 0\n    running = True\n    prune_amount = 0.99\n    prune_model(net, prune_amount, True)\n    while running:\n        net.train()\n        for sample in retain_loader:\n            inputs = sample[\"image\"]\n            targets = sample[\"age_group\"]\n            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n            \n            # Get target distribution\n            with torch.no_grad():\n                original_outputs = initial_net(inputs)\n                preds = torch.log_softmax(original_outputs, dim=1)\n            \n            optimizer.zero_grad()\n            outputs = net(inputs)\n            loss = entropy_loss_fn(outputs, targets, preds, class_weights)\n            loss.backward()\n            optimizer.step()\n\n            num_iters += 1\n            # Stop at max iters\n            if num_iters > max_iters:\n                running = False\n                break\n        \n    net.eval()","metadata":{"execution":{"iopub.status.busy":"2023-12-01T01:01:26.789734Z","iopub.execute_input":"2023-12-01T01:01:26.790684Z","iopub.status.idle":"2023-12-01T01:01:26.808189Z","shell.execute_reply.started":"2023-12-01T01:01:26.790648Z","shell.execute_reply":"2023-12-01T01:01:26.807240Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"if os.path.exists('/kaggle/input/neurips-2023-machine-unlearning/empty.txt'):\n    # mock submission\n    net = resnet18(weights=None, num_classes=10)\n    for k, m in enumerate(net.modules()):\n        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n            prune.l1_unstructured(m, name=\"weight\", amount=0.95)\n            prune.remove(m, 'weight')\n            \n    print(m)\n    subprocess.run('touch submission.zip', shell=True)\nelse:\n    \n    # Note: it's really important to create the unlearned checkpoints outside of the working directory \n    # as otherwise this notebook may fail due to running out of disk space.\n    # The below code saves them in /kaggle/tmp to avoid that issue.\n    class_weights_fname = \"/kaggle/input/neurips-2023-machine-unlearning/age_class_weights.json\"\n    with open(class_weights_fname) as f:\n        # Returns JSON object as a dictionary(?)\n        class_weights = json.load(f)\n\n    # Remove any dictionary layers, if there are any(?)\n    while isinstance(class_weights, dict):\n        if len(class_weights) > 1:\n            # Assume each key maps to one weight, in the correct order\n            class_weights = list(class_weights.values())\n            break\n        for _, class_weights in class_weights.items():\n            # Strip away a dict layer and handle its contents, using the\n            # value from the first key in the dict only.\n            break\n\n    # We should now have a list\n    # if not isinstance(class_weights, list):\n    #     raise ValueError(f\"class_weights is a {type(class_weights)}, not a list\")\n\n    # Convert list of weights into a tensor\n    class_weights = torch.tensor(class_weights).to(DEVICE, dtype=torch.float32)\n    # The JSON file actually contains number of occurances. To correct for imbalance, the\n    # weighting should be the reciprocal of the count.\n    class_weights = class_weights ** -0.1\n    \n    os.makedirs('/kaggle/tmp', exist_ok=True)\n    retain_loader, forget_loader, validation_loader = get_dataset(64)\n    net = resnet18(weights=None, num_classes=10)\n    net.to(DEVICE)\n    for i in range(512):\n        net.load_state_dict(torch.load('/kaggle/input/neurips-2023-machine-unlearning/original_model.pth'))\n        unlearning(net, retain_loader, forget_loader, validation_loader, class_weights=class_weights)\n        net = net.to(torch.half)\n        state = net.state_dict()\n        torch.save(state, f'/kaggle/tmp/unlearned_checkpoint_{i}.pth')\n        net = net.to(torch.float)\n        \n    # Ensure that submission.zip will contain exactly 512 checkpoints \n    # (if this is not the case, an exception will be thrown).\n    unlearned_ckpts = os.listdir('/kaggle/tmp')\n    if len(unlearned_ckpts) != 512:\n        raise RuntimeError('Expected exactly 512 checkpoints. The submission will throw an exception otherwise.')\n        \n    subprocess.run('zip submission.zip /kaggle/tmp/*.pth', shell=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T01:01:26.809463Z","iopub.execute_input":"2023-12-01T01:01:26.809908Z","iopub.status.idle":"2023-12-01T01:01:28.502363Z","shell.execute_reply.started":"2023-12-01T01:01:26.809875Z","shell.execute_reply":"2023-12-01T01:01:28.501376Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Linear(in_features=512, out_features=10, bias=True)\n","output_type":"stream"}]}]}